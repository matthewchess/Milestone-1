{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all the data/usda_* into one dataframe\n",
    "usda = pd.concat([pd.read_csv(f'data/usda_data_{year}.csv')  for year in range(1950, 2024)])\n",
    "\n",
    "# combine all the data/weather_* into one dataframe\n",
    "weather = pd.concat([pd.read_csv(f'data/weather_data_{year}.csv') for year in range(2000, 2021)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usda.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the average value in udsa data for each year\n",
    "usda.groupby('year')['Value'].mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll select the columns that I need for analysis. I will focus on year, county_name and Value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the average value in udsa data for each year\n",
    "usda.groupby('year')['Value'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "usda['year'] = pd.to_datetime(usda['year'],\n",
    "               format='%Y')\n",
    "usda_subset = usda[['year','county_name','Value']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usda_subset['county_name'].nunique() #85 Michigan has 83"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is that the data set has 85 and Michigan only has 83. The additional two are 'OTHER COUNTIES' and 'OTHER (COMBINED) COUNTIES'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usda_subset_other_combined = usda_subset[usda_subset['county_name'] == 'OTHER (COMBINED) COUNTIES']\n",
    "usda_subset_other_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 144 records that contain 'OTHER (COMBINED) COUNTIES'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usda_subset_other = usda_subset[usda_subset['county_name'] == 'OTHER COUNTIES']\n",
    "usda_subset_other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\tyear\tcounty_name\tValue\n",
    "60\t2020-01-01\tOTHER COUNTIES\t151.0\n",
    "57\t2021-01-01\tOTHER COUNTIES\t150.2\n",
    "59\t2022-01-01\tOTHER COUNTIES\t163.2\n",
    "57\t2023-01-01\tOTHER COUNTIES\t166.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 4 records (years) that contain 'OTHER COUNTIES'. We will focus on the year 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usda_other = usda[usda['county_name'] == 'OTHER COUNTIES']\n",
    "usda_other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The asd_desc is null for all 4 records. location_desc for all 4 records is \"MICHIGAN, OTHER COUNTIES\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "usda_subset_2020=usda_subset[usda_subset['year'] == '2020-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usda_subset_2020['county_name'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 61 counties in the data set for 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usda_subset_2020['county_name'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "array(['DELTA', 'DICKINSON', 'MENOMINEE', 'ANTRIM', 'BENZIE',\n",
    "       'CHARLEVOIX', 'EMMET', 'GRAND TRAVERSE', 'LEELANAU', 'MANISTEE',\n",
    "       'WEXFORD', 'ALCONA', 'ALPENA', 'IOSCO', 'OGEMAW', 'OTSEGO',\n",
    "       'PRESQUE ISLE', 'MASON', 'MUSKEGON', 'NEWAYGO', 'OCEANA',\n",
    "       'GLADWIN', 'ISABELLA', 'MECOSTA', 'MIDLAND', 'MONTCALM', 'ARENAC',\n",
    "       'BAY', 'HURON', 'SAGINAW', 'SANILAC', 'TUSCOLA', 'ALLEGAN',\n",
    "       'BERRIEN', 'CASS', 'KALAMAZOO', 'KENT', 'OTTAWA', 'VAN BUREN',\n",
    "       'BARRY', 'BRANCH', 'CALHOUN', 'CLINTON', 'EATON', 'HILLSDALE',\n",
    "       'INGHAM', 'IONIA', 'JACKSON', 'ST JOSEPH', 'SHIAWASSEE', 'GENESEE',\n",
    "       'LAPEER', 'LENAWEE', 'LIVINGSTON', 'MACOMB', 'MONROE', 'OAKLAND',\n",
    "       'ST CLAIR', 'WASHTENAW', 'WAYNE', 'OTHER COUNTIES'], dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "sns.histplot(data=usda_subset, x=\"Value\", hue=\"county_name\", legend=False)\n",
    "plt.xlabel('BU/Acre')\n",
    "plt.title('Bushels/Acre by County for Past 2 Decades')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust figure size for better readability\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Create the scatter plot\n",
    "g = sns.scatterplot(data=usda_subset_2020, x=\"county_name\", y=\"Value\", hue=\"year\")\n",
    "\n",
    "# Rotate the x-axis labels for readability\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Counties')\n",
    "plt.ylabel('BU/Acre')\n",
    "plt.title('Bushels/Acre by County for 2020')\n",
    "\n",
    "# Adjust the legend positioning\n",
    "#g.legend(loc='center left', bbox_to_anchor=(1.25, 0.5), ncol=1)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that perhaps the \"Other Counties\" was a dumping ground for smaller quanities collected over several counties. There is only one dot respresenting the one entry for the year 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was there really 3 entries for 1960 for \"Other (Combined) Counties\"? A dataset containing the 1960 entries will be created and investigated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "usda_subset_1960=usda_subset[usda_subset['year'] == '1960-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust figure size for better readability\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Create the scatter plot\n",
    "g = sns.scatterplot(data=usda_subset_1960, x=\"county_name\", y=\"Value\", hue=\"year\")\n",
    "\n",
    "# Rotate the x-axis labels for readability\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Counties')\n",
    "plt.ylabel('BU/Acre')\n",
    "plt.title('Bushels/Acre by County for 1960')\n",
    "\n",
    "# Adjust the legend positioning\n",
    "#g.legend(loc='center left', bbox_to_anchor=(1.25, 0.5), ncol=1)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure enough there are three dots over the \"Other (Combined) Counties\" mark. What does it look like when we look at the entered values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usda_1960 = usda[(usda['year'] == '1960-01-01') & (usda['county_name'] == 'OTHER (COMBINED) COUNTIES')]\n",
    "usda_1960"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 3 entries for 1960 have the following asd_desc: \"UPPER PENINSULA\", \"NORTHWEST\", and \"NORTHEAST\". The location_desc has these entries: \"MICHIGAN, UPPER PENINSULA, OTHER (COMBINED) COUNTIES\", \"MICHIGAN, NORTHWEST, OTHER (COMBINED) COUNTIES\", and \"MICHIGAN, NORTHEAST, OTHER (COMBINED) COUNTIES\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = usda_subset_1960.groupby('county_name')['Value'].sum().reset_index()\n",
    "print(grouped_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total Value for 1960 \"OTHER (COMBINED) COUNTIES\" was 130.6\n",
    "A similiar dumping ground value when compared to the single \"OTHER COUNTIES\" in 2020.\n",
    "\n",
    "2019 seemed to have a lot of entries for \"OTHER (COMBINED) COUNTIES\". What is going on?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usda_2019 = usda[(usda['year'] == '2019-01-01') & (usda['county_name'] == 'OTHER (COMBINED) COUNTIES')]\n",
    "usda_2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 7 entries for 2019 have the following asd_desc: \"UPPER PENINSULA\", \"NORTHWEST\", \"NORTHEAST\",\"WEST CENTRAL\", \"CENTRAL\", \"SOUTHWEST\", and \"SOUTH CENTRAL\". The location_desc has these entries: \"MICHIGAN, UPPER PENINSULA, OTHER (COMBINED) COUNTIES\", \"MICHIGAN, NORTHWEST, OTHER (COMBINED) COUNTIES\", \"MICHIGAN, NORTHEAST, OTHER (COMBINED) COUNTIES\",\"MICHIGAN, WEST CENTRAL, OTHER (COMBINED) COUNTIES\", \"MICHIGAN, CENTRAL, OTHER (COMBINED) COUNTIES\", \"MICHIGAN, SOUTHWEST, OTHER (COMBINED) COUNTIES\", and \"MICHIGAN, SOUTH CENTRAL, OTHER (COMBINED) COUNTIES\".\n",
    "\n",
    "How many years and how many entries are we looking at that are like this? It looks like 2019 was the worst with 7 regions identified. The amount of bushels/acre is  886.3. This is considerably more than 1960."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = usda.groupby(['year', 'county_name']).agg(\n",
    "    count=('county_name', 'count'),\n",
    "    sum=('Value', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "filtered_df = grouped_df[grouped_df['count'] > 1]\n",
    "# Sort by 'sum' in descending order\n",
    "filtered_df = filtered_df.sort_values(by='sum', ascending=False)\n",
    "\n",
    "print(filtered_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "             year                county_name  count    sum\n",
    "4768 2019-01-01  OTHER (COMBINED) COUNTIES      7  886.3\n",
    "4610 2016-01-01  OTHER (COMBINED) COUNTIES      6  824.1\n",
    "4551 2015-01-01  OTHER (COMBINED) COUNTIES      6  823.4\n",
    "4670 2017-01-01  OTHER (COMBINED) COUNTIES      5  655.8\n",
    "3942 2005-01-01  OTHER (COMBINED) COUNTIES      5  576.0\n",
    "3638 2000-01-01  OTHER (COMBINED) COUNTIES      6  565.0\n",
    "4001 2006-01-01  OTHER (COMBINED) COUNTIES      5  555.0\n",
    "3589 1999-01-01  OTHER (COMBINED) COUNTIES      5  533.0\n",
    "4061 2007-01-01  OTHER (COMBINED) COUNTIES      6  502.0\n",
    "3881 2004-01-01  OTHER (COMBINED) COUNTIES      5  453.0\n",
    "4120 2008-01-01  OTHER (COMBINED) COUNTIES      5  442.0\n",
    "4493 2014-01-01  OTHER (COMBINED) COUNTIES      4  436.4\n",
    "3820 2003-01-01  OTHER (COMBINED) COUNTIES      4  398.0\n",
    "4717 2018-01-01  OTHER (COMBINED) COUNTIES      3  397.1\n",
    "4248 2010-01-01  OTHER (COMBINED) COUNTIES      3  385.7\n",
    "3464 1997-01-01  OTHER (COMBINED) COUNTIES      4  345.0\n",
    "3691 2001-01-01  OTHER (COMBINED) COUNTIES      6  327.0\n",
    "3757 2002-01-01  OTHER (COMBINED) COUNTIES      3  308.0\n",
    "4183 2009-01-01  OTHER (COMBINED) COUNTIES      3  303.0\n",
    "4314 2011-01-01  OTHER (COMBINED) COUNTIES      3  288.7\n",
    "4432 2013-01-01  OTHER (COMBINED) COUNTIES      2  245.4\n",
    "4377 2012-01-01  OTHER (COMBINED) COUNTIES      3  239.2\n",
    "3530 1998-01-01  OTHER (COMBINED) COUNTIES      3  230.0\n",
    "3406 1996-01-01  OTHER (COMBINED) COUNTIES      3  206.0\n",
    "3340 1995-01-01  OTHER (COMBINED) COUNTIES      2  180.0\n",
    "1232 1967-01-01  OTHER (COMBINED) COUNTIES      3  172.8\n",
    "2190 1980-01-01  OTHER (COMBINED) COUNTIES      2  160.6\n",
    "1128 1965-01-01  OTHER (COMBINED) COUNTIES      3  159.3\n",
    "1180 1966-01-01  OTHER (COMBINED) COUNTIES      3  152.5\n",
    "2044 1978-01-01  OTHER (COMBINED) COUNTIES      2  150.0\n",
    "2117 1979-01-01  OTHER (COMBINED) COUNTIES      2  147.4\n",
    "952  1962-01-01  OTHER (COMBINED) COUNTIES      3  144.8\n",
    "1004 1963-01-01  OTHER (COMBINED) COUNTIES      3  142.2\n",
    "900  1961-01-01  OTHER (COMBINED) COUNTIES      3  139.2\n",
    "848  1960-01-01  OTHER (COMBINED) COUNTIES      3  130.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When comparing to the rest of the entries in 2019. How bad does it look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "usda_subset_2019=usda_subset[usda_subset['year'] == '2019-01-01']\n",
    "grouped_df_2019 = usda_subset_2019.groupby(['year', 'county_name']).agg(\n",
    "    count=('county_name', 'count'),\n",
    "    sum=('Value', 'sum')\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust figure size for better readability\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Create the scatter plot\n",
    "g = sns.scatterplot(data=grouped_df_2019, x=\"county_name\", y=\"sum\", hue=\"year\")\n",
    "\n",
    "# Rotate the x-axis labels for readability\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Counties')\n",
    "plt.ylabel('BU/Acre')\n",
    "plt.title('Bushels/Acre by County for 2019')\n",
    "\n",
    "# Adjust the legend positioning\n",
    "#g.legend(loc='center left', bbox_to_anchor=(1.25, 0.5), ncol=1)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the counties for 2019 produced no more than 200 bushels/acre. As expected, \"OTHER (COMBINED) COUNTIES\" is the highest in the chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GDD calculation function\n",
    "def calculate_gdd(df, base_temp=50, upper_temp=86):\n",
    "    \"\"\"\n",
    "    Calculate Growing Degree Days (GDD) for corn.\n",
    "    \"\"\"\n",
    "    df['TMAX'] = df['TMAX'].clip(lower=base_temp, upper=upper_temp)\n",
    "    df['TMIN'] = df['TMIN'].clip(lower=base_temp)\n",
    "    df['TAVG'] = (df['TMAX'] + df['TMIN']) / 2\n",
    "    df['GDD'] = df['TAVG'] - base_temp\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = calculate_gdd(weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare average yield for usda_data_1955 and usda_data_2015\n",
    "usda_1955 = pd.read_csv('data/usda_data_1955.csv')\n",
    "usda_2015 = pd.read_csv('data/usda_data_2015.csv')\n",
    "\n",
    "usda_1955['Value'].mean(), usda_2015['Value'].mean()\n",
    "\n",
    "# whats the std deviation of yield for those years\n",
    "usda_1955['Value'].std(), usda_2015['Value'].std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare total gdd for weather_data_1955 and weather_data_2015 for county_ansi=161\n",
    "weather_2014 = pd.read_csv('data/weather_data_2014.csv')\n",
    "weather_2023 = pd.read_csv('data/weather_data_2023.csv')\n",
    "\n",
    "weather_2014 = calculate_gdd(weather_2014)\n",
    "weather_2023 = calculate_gdd(weather_2023)\n",
    "\n",
    "weather_2014[weather_2014['county_ansi'] == 161]['GDD'].sum(), weather_2023[weather_2023['county_ansi'] == 161]['GDD'].sum()\n",
    "\n",
    "# Do this for all years between 1950 and 1959\n",
    "gdd = []\n",
    "for year in range(1950, 1960):\n",
    "    weather = pd.read_csv(f'data/weather_data_{year}.csv')\n",
    "    weather = calculate_gdd(weather)\n",
    "    gdd.append(weather[weather['county_ansi'] == 161]['GDD'].sum())\n",
    "\n",
    "gdd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all weather data into a single DataFrame\n",
    "weather_data = pd.concat([pd.read_csv(f'data/weather_data_{year}.csv') for year in range(1990, 2024)])\n",
    "# Calculate GDD for each day\n",
    "weather_data = calculate_gdd(weather_data)\n",
    "weather_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'date' to datetime if it's not already\n",
    "weather_data['date'] = pd.to_datetime(weather_data['date'])\n",
    "\n",
    "# Extract year and month\n",
    "weather_data['year'] = weather_data['date'].dt.year\n",
    "weather_data['month'] = weather_data['date'].dt.month\n",
    "\n",
    "# Aggregate GDD by county and year\n",
    "gdd_annual = weather_data.groupby(['state_ansi', 'county_ansi', 'year'])['GDD'].sum().reset_index()\n",
    "gdd_annual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all USDA data into a single DataFrame usda_data\n",
    "usda_data = pd.concat([pd.read_csv(f'data/usda_data_{year}.csv') for year in range(1990, 2024)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I added some lines to clean the data, most files are fine but I am looking for the 'YIELD' data and skipping any suppressed data\n",
    "\n",
    "# Filter for the relevant data (e.g., 'YIELD' in 'statisticcat_desc')\n",
    "corn_data = usda_data[usda_data['statisticcat_desc'] == 'YIELD']\n",
    "\n",
    "# Convert 'Value' to numeric, removing any commas or missing values\n",
    "corn_data['Value'] = corn_data['Value'].replace(',', '', regex=True)\n",
    "corn_data = corn_data[corn_data['Value'] != '(D)']  # Remove suppressed data\n",
    "corn_data['Value'] = pd.to_numeric(corn_data['Value'])\n",
    "\n",
    "# Select relevant columns\n",
    "corn_data = corn_data[['state_ansi', 'county_ansi', 'year', 'Value']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the GDD and corn yield data to make a single DataFrame\n",
    "\n",
    "# Merge on 'state_ansi', 'county_ansi', and 'year'\n",
    "merged_data = pd.merge(gdd_annual, corn_data, on=['state_ansi', 'county_ansi', 'year'])\n",
    "# Remove entries with zero or NaN yields\n",
    "merged_data = merged_data[merged_data['Value'] > 0]\n",
    "# Remove entries with zero or NaN yields\n",
    "merged_data = merged_data[merged_data['GDD'] > 0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump merged_data to csv to test\n",
    "merged_data.to_csv('merged_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate IQR for 'GDD' and 'Value' columns to identify outliers\n",
    "Q1_GDD = merged_data['GDD'].quantile(0.25)\n",
    "Q3_GDD = merged_data['GDD'].quantile(0.75)\n",
    "IQR_GDD = Q3_GDD - Q1_GDD\n",
    "\n",
    "Q1_Value = merged_data['Value'].quantile(0.25)\n",
    "Q3_Value = merged_data['Value'].quantile(0.75)\n",
    "IQR_Value = Q3_Value - Q1_Value\n",
    "\n",
    "# Define bounds for outliers in 'GDD' and 'Value'\n",
    "lower_bound_GDD = Q1_GDD - 1.5 * IQR_GDD\n",
    "upper_bound_GDD = Q3_GDD + 1.5 * IQR_GDD\n",
    "\n",
    "lower_bound_Value = Q1_Value - 1.5 * IQR_Value\n",
    "upper_bound_Value = Q3_Value + 1.5 * IQR_Value\n",
    "\n",
    "# Filter out the outliers\n",
    "merged_data = merged_data[\n",
    "    (merged_data['GDD'] >= lower_bound_GDD) & (merged_data['GDD'] <= upper_bound_GDD) &\n",
    "    (merged_data['Value'] >= lower_bound_Value) & (merged_data['Value'] <= upper_bound_Value)\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scatter plot of GDD vs. corn yield\n",
    "sns.lmplot(x='GDD', y='Value', data=merged_data, scatter_kws={'s': 1}, hue='year', palette='viridis', fit_reg=False)\n",
    "plt.title('Correlation between GDD and Corn Yield')\n",
    "plt.xlabel('Growing Degree Days (GDD)')\n",
    "plt.ylabel('Corn Yield (bu/acre)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the same chart with PRCP instead of GDD\n",
    "\n",
    "# Calculate total precipitation for the growing season\n",
    "weather_data['PRCP'] = weather_data['PRCP'].clip(lower=0)\n",
    "prcp_annual = weather_data.groupby(['state_ansi', 'county_ansi', 'year'])['PRCP'].sum().reset_index()\n",
    "\n",
    "# Merge on 'state_ansi', 'county_ansi', and 'year'\n",
    "merged_data_prcp = pd.merge(prcp_annual, corn_data, on=['state_ansi', 'county_ansi', 'year'])\n",
    "# Remove entries with zero or NaN yields\n",
    "merged_data_prcp = merged_data_prcp[merged_data_prcp['Value'] > 0]\n",
    "# Remove entries with zero or NaN yields\n",
    "merged_data_prcp = merged_data_prcp[merged_data_prcp['PRCP'] > 0]\n",
    "\n",
    "# Scatter plot with regression line\n",
    "sns.lmplot(x='PRCP', y='Value', data=merged_data_prcp, scatter_kws={'s': 0.5})\n",
    "plt.title('Correlation between PRCP and Corn Yield')\n",
    "plt.xlabel('Total Precipitation (mm)')\n",
    "plt.ylabel('Corn Yield (bu/acre)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add precipitation data to the merged dataframe\n",
    "prcp_annual = weather_data.groupby(['state_ansi', 'county_ansi', 'year'])['PRCP'].sum().reset_index()\n",
    "merged_data = pd.merge(merged_data, prcp_annual, on=['state_ansi', 'county_ansi', 'year'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Define independent variables and dependent variable\n",
    "X = merged_data[['GDD', 'PRCP']]\n",
    "y = merged_data['Value']\n",
    "\n",
    "# Add a constant term for the intercept\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the regression model\n",
    "model = sm.OLS(y, X).fit()\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used chatGPT to add some centext to the OLS model.\n",
    "\n",
    "Let's break down the key components of your OLS (Ordinary Least Squares) regression results:\n",
    "\n",
    "### 1. **Model Overview:**\n",
    "   - **Dep. Variable (Dependent Variable):** This is the variable you're trying to predict, in this case, labeled as \"Value.\"\n",
    "   - **Model:** OLS, meaning this is a simple linear regression model using the least squares method.\n",
    "   - **No. Observations:** 4673 observations (data points) were used in this regression.\n",
    "\n",
    "### 2. **R-squared:**\n",
    "   - **R-squared (0.099):** This indicates that the model explains about **9.9% of the variance** in the dependent variable (\"Value\"). This is a relatively low R-squared value, meaning that most of the variability in the data is not explained by the model.\n",
    "   - **Adj. R-squared (0.098):** This is a slightly adjusted version of R-squared that accounts for the number of predictors in the model. It's still low, meaning the model is not very strong at explaining the data.\n",
    "\n",
    "### 3. **F-statistic and p-value (Prob F-statistic):**\n",
    "   - **F-statistic (255.8):** This tests the overall significance of the model. A higher F-statistic generally means the model is a good fit.\n",
    "   - **Prob(F-statistic) (3.88e-106):** The p-value associated with the F-statistic is incredibly small (close to zero), meaning the model is statistically significant overall. So, even though the model doesn't explain much variance (low R-squared), it's still better than having no model at all.\n",
    "\n",
    "### 4. **Coefficients (coef), t-values, and p-values:**\n",
    "   These are the estimated effects of each independent variable on the dependent variable.\n",
    "   - **const (Intercept) (0.0849):** This is the value of the dependent variable (\"Value\") when all independent variables (GDD, PRCP) are zero. The p-value for this is 0.983, which means it's not statistically significant.\n",
    "   - **GDD (0.0274):** This means that for each unit increase in GDD (an independent variable), the dependent variable increases by approximately 0.0274 units. The p-value is 0.000, so this is a statistically significant effect.\n",
    "   - **PRCP (1.9657):** This means that for each unit increase in PRCP, the dependent variable increases by 1.9657 units. The p-value is also 0.000, indicating a highly significant effect.\n",
    "\n",
    "### 5. **Standard Error (std err):**\n",
    "   - The standard errors give a measure of the variability in the coefficient estimates. For example, the standard error for GDD is 0.002, which indicates a small amount of variability, suggesting a reliable estimate.\n",
    "\n",
    "### 6. **t-value and p-value:**\n",
    "   - **t-value:** This tells you how many standard errors the coefficient is away from zero. Higher absolute t-values indicate that the corresponding coefficient is statistically significant.\n",
    "   - **p-value:** This tells you whether the coefficient is statistically significant. For GDD and PRCP, the p-values are 0.000, meaning both are statistically significant. The intercept is not significant (p-value = 0.983).\n",
    "\n",
    "### 7. **Durbin-Watson (0.422):**\n",
    "   - This statistic tests for autocorrelation in the residuals (errors). A value close to 2 means there is no autocorrelation. Here, 0.422 is quite low, suggesting the possibility of positive autocorrelation, which might need further investigation.\n",
    "\n",
    "### 8. **Omnibus and Jarque-Bera (JB) tests:**\n",
    "   - These are tests for normality of the residuals. The p-values for these tests are very small (Prob(Omnibus) = 0.000, Prob(JB) = 8.09e-35), indicating that the residuals are not normally distributed, which could be a problem depending on the assumptions of your model.\n",
    "\n",
    "### 9. **Condition Number (1.66e+04):**\n",
    "   - A high condition number suggests multicollinearity, meaning that the independent variables are highly correlated. The condition number here (16,600) is quite high, indicating potential multicollinearity issues.\n",
    "\n",
    "### Conclusion:\n",
    "- The model is statistically significant, but it doesn't explain much of the variability in the dependent variable (low R-squared).\n",
    "- The variables **GDD** and **PRCP** have a significant impact on the dependent variable (\"Value\").\n",
    "- However, there may be issues with autocorrelation (Durbin-Watson) and multicollinearity (high condition number), and the residuals don't appear to be normally distributed (Omnibus and JB tests). You might need to address these issues if you want to improve the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning up 'Value' to make sure we have something to work with\n",
    "usda_data['Value'] = usda_data['Value'].replace(',', '', regex=True)\n",
    "usda_data['Value'] = usda_data['Value'].replace({'(D)': np.nan, '(Z)': np.nan, '(NA)': np.nan, '': np.nan})\n",
    "usda_data['Value'] = pd.to_numeric(usda_data['Value'], errors='coerce')\n",
    "\n",
    "# Filter for the relevant data (e.g., 'YIELD' in 'statisticcat_desc')\n",
    "corn_data = usda_data[usda_data['statisticcat_desc'] == 'YIELD']\n",
    "\n",
    "# Select relevant columns, including 'county_name'\n",
    "corn_data = corn_data[['state_ansi', 'county_ansi', 'year', 'Value', 'county_name']]\n",
    "\n",
    "# Drop rows with missing values in 'Value' or 'county_name'\n",
    "corn_data.dropna(subset=['Value', 'county_name'], inplace=True)\n",
    "\n",
    "# Ensure 'year' is integer\n",
    "corn_data['year'] = corn_data['year'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge on 'state_ansi', 'county_ansi', and 'year'\n",
    "merged_data = pd.merge(gdd_annual, corn_data, on=['state_ansi', 'county_ansi', 'year'])\n",
    "\n",
    "# Drop any rows with missing values\n",
    "merged_data.dropna(subset=['GDD', 'Value'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average yield per county\n",
    "avg_yield_per_county = merged_data.groupby(['county_ansi', 'county_name'])['Value'].mean().reset_index()\n",
    "\n",
    "# Sort and select top 5 counties\n",
    "top_counties = avg_yield_per_county.sort_values(by='Value', ascending=False).head(5)\n",
    "\n",
    "# Filter data for these counties\n",
    "top_county_codes = top_counties['county_ansi'].unique()\n",
    "top_county_names = top_counties['county_name'].unique()\n",
    "top_counties_data = merged_data[merged_data['county_ansi'].isin(top_county_codes)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# We can use a different plotting lib I just wnated something quick\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "num_counties = len(top_county_codes)\n",
    "fig, axes = plt.subplots(nrows=num_counties, ncols=1, figsize=(12, num_counties * 4), sharex=True)\n",
    "\n",
    "# Ensure axes is iterable\n",
    "if num_counties == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, county_code, county_name in zip(axes, top_county_codes, top_county_names):\n",
    "    county_data = top_counties_data[top_counties_data['county_ansi'] == county_code]\n",
    "    county_data = county_data.sort_values('year')\n",
    "\n",
    "    # Plot Yield\n",
    "    ax.plot(county_data['year'], county_data['Value'], color='blue', marker='o', label='Corn Yield (bu/acre)')\n",
    "    ax.set_ylabel('Corn Yield (bu/acre)', color='blue')\n",
    "    ax.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "    # Create a second y-axis for GDD\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.plot(county_data['year'], county_data['GDD'], color='green', marker='x', label='GDD')\n",
    "    ax2.set_ylabel('GDD', color='green')\n",
    "    ax2.tick_params(axis='y', labelcolor='green')\n",
    "\n",
    "    # Title with county name\n",
    "    ax.set_title(f'Corn Yield and GDD Over Time - {county_name}')\n",
    "\n",
    "    # Add legends\n",
    "    lines, labels = ax.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax.legend(lines + lines2, labels + labels2, loc='upper left')\n",
    "\n",
    "# Set common x-label\n",
    "plt.xlabel('Year')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average yield per county over the entire period\n",
    "average_yield = merged_data.groupby('county_name')['Value'].mean().reset_index()\n",
    "\n",
    "# Sort counties by average yield in descending order\n",
    "average_yield_sorted = average_yield.sort_values(by='Value', ascending=False)\n",
    "\n",
    "# Top 10 counties\n",
    "print(\"Top 10 Counties by Average Corn Yield:\")\n",
    "print(average_yield_sorted.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot of average yields for top 10 counties\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(average_yield_sorted['county_name'].head(10), average_yield_sorted['Value'].head(10), color='skyblue')\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('County')\n",
    "plt.ylabel('Average Corn Yield (bu/acre)')\n",
    "plt.title('Top 10 Counties by Average Corn Yield')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I was wondering if we pick the best based how consistent are they in beating the state average\n",
    "\n",
    "# Calculate the state average yield for each year\n",
    "state_average_yield = merged_data.groupby('year')['Value'].mean().reset_index(name='state_avg_yield')\n",
    "\n",
    "# Merge state average back into merged_data\n",
    "merged_data_with_state_avg = pd.merge(merged_data, state_average_yield, on='year')\n",
    "\n",
    "# Create a flag for whether the county's yield is above the state average each year\n",
    "merged_data_with_state_avg['above_state_avg'] = merged_data_with_state_avg['Value'] > merged_data_with_state_avg['state_avg_yield']\n",
    "\n",
    "# Calculate the percentage of years each county was above the state average\n",
    "county_performance = merged_data_with_state_avg.groupby('county_name')['above_state_avg'].mean().reset_index()\n",
    "\n",
    "# Convert to percentage\n",
    "county_performance['percent_above_state_avg'] = county_performance['above_state_avg'] * 100\n",
    "\n",
    "# Sort counties by percentage in descending order\n",
    "county_performance_sorted = county_performance.sort_values(by='percent_above_state_avg', ascending=False)\n",
    "\n",
    "# Display the top 10 counties\n",
    "print(\"Top 10 Counties by Consistency in Exceeding State Average Yield:\")\n",
    "print(county_performance_sorted.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot of top 10 counties by percentage of years above state average\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(county_performance_sorted['county_name'].head(10), county_performance_sorted['percent_above_state_avg'].head(10), color='orange')\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('County')\n",
    "plt.ylabel('Percentage of Years Above State Average (%)')\n",
    "plt.title('Top 10 Counties Consistently Exceeding State Average Yield')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about finding the best improved county over time wiht slope\n",
    "\n",
    "# Pulling this in for linregress regression\n",
    "from scipy.stats import linregress\n",
    "\n",
    "# Prepare a DataFrame to store slopes\n",
    "county_slopes = []\n",
    "\n",
    "for county in merged_data['county_name'].unique():\n",
    "    county_data = merged_data[merged_data['county_name'] == county]\n",
    "    if len(county_data['year'].unique()) > 1:  # Ensure there's enough data\n",
    "        slope, intercept, r_value, p_value, std_err = linregress(county_data['year'], county_data['Value'])\n",
    "        county_slopes.append({'county_name': county, 'slope': slope, 'p_value': p_value})\n",
    "    else:\n",
    "        county_slopes.append({'county_name': county, 'slope': np.nan, 'p_value': np.nan})\n",
    "\n",
    "# Convert to DataFrame\n",
    "slopes_df = pd.DataFrame(county_slopes)\n",
    "\n",
    "# Remove counties with NaN slopes\n",
    "slopes_df.dropna(subset=['slope'], inplace=True)\n",
    "\n",
    "# Sort counties by slope\n",
    "slopes_df_sorted = slopes_df.sort_values(by='slope', ascending=False)\n",
    "\n",
    "print(\"Top 10 Counties by Yield Improvement Over Time:\")\n",
    "print(slopes_df_sorted.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets looks at the counties that have been the most consistent in their yield over time\n",
    "\n",
    "# Calculate standard deviation of yield per county\n",
    "yield_variability = merged_data.groupby('county_name')['Value'].std().reset_index(name='yield_std')\n",
    "\n",
    "# Sort counties by yield_std in ascending order\n",
    "yield_variability_sorted = yield_variability.sort_values(by='yield_std')\n",
    "\n",
    "print(\"Top 10 Counties with Least Yield Variability:\")\n",
    "print(yield_variability_sorted.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look at the counties with the highest average gdd\n",
    "\n",
    "# Calculate average GDD per county\n",
    "avg_gdd_per_county = merged_data.groupby('county_name')['GDD'].mean().reset_index()\n",
    "\n",
    "# Sort counties by average GDD in descending order\n",
    "avg_gdd_per_county_sorted = avg_gdd_per_county.sort_values(by='GDD', ascending=False)\n",
    "\n",
    "print(\"Top 10 Counties by Average Growing Degree Days:\")\n",
    "print(avg_gdd_per_county_sorted.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the average GDD\n",
    "county_metrics['GDD_norm'] = scaler.fit_transform(county_metrics[['GDD']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use county_name to merge all the other DFs\n",
    "county_metrics = average_yield[['county_name', 'Value']].merge(\n",
    "    county_performance[['county_name', 'percent_above_state_avg']], on='county_name').merge(\n",
    "    slopes_df[['county_name', 'slope']], on='county_name').merge(\n",
    "    yield_variability[['county_name', 'yield_std']], on='county_name').merge(\n",
    "    avg_gdd_per_county[['county_name', 'GDD']], on='county_name')\n",
    "\n",
    "# Normalize the metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "county_metrics[['avg_yield_norm', 'percent_above_avg_norm', 'slope_norm', 'yield_std_norm', 'avg_gdd_per_county']] = scaler.fit_transform(\n",
    "    county_metrics[['Value', 'percent_above_state_avg', 'slope', 'yield_std', 'GDD']])\n",
    "\n",
    "# Invert yield_std_norm so that lower variability scores higher\n",
    "county_metrics['yield_std_norm'] = 1 - county_metrics['yield_std_norm']\n",
    "\n",
    "# I assigned weights to each metric but nothing scientific\n",
    "# Should come back to this later\n",
    "county_metrics['total_score'] = (\n",
    "    county_metrics['avg_yield_norm'] * 0.4 +\n",
    "    county_metrics['percent_above_avg_norm'] * 0.3 +\n",
    "    county_metrics['slope_norm'] * 0.2 +\n",
    "    county_metrics['yield_std_norm'] * 0.1 +\n",
    "    county_metrics['avg_gdd_per_county'] * 0.1\n",
    ")\n",
    "\n",
    "# Sort counties by total_score\n",
    "county_metrics_sorted = county_metrics.sort_values(by='total_score', ascending=False)\n",
    "\n",
    "print(\"Top 10 Counties by Composite Score:\")\n",
    "print(county_metrics_sorted[['county_name', 'total_score']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot of top 10 counties by composite score\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(county_metrics_sorted['county_name'].head(10), county_metrics_sorted['total_score'].head(10), color='purple')\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('County')\n",
    "plt.ylabel('Composite Score')\n",
    "plt.title('Top 10 Counties by Composite Score')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Prepare data for heatmap\n",
    "metrics_for_heatmap = county_metrics_sorted[['county_name', 'avg_yield_norm', 'percent_above_avg_norm', 'slope_norm', 'yield_std_norm', 'total_score']].set_index('county_name')\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(metrics_for_heatmap.head(10), annot=True, cmap='YlGnBu')\n",
    "plt.title('Normalized Metrics and Total Score for Top 10 Counties')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
