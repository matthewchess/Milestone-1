{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.stats import linregress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our Discovery work for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all the data/usda_* into one dataframe\n",
    "usda = pd.concat([pd.read_csv(f'data/usda_data_{year}.csv')  for year in range(1950, 2024)])\n",
    "\n",
    "# combine all the data/weather_* into one dataframe\n",
    "weather = pd.concat([pd.read_csv(f'data/weather_data_{year}.csv') for year in range(2000, 2021)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usda.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the average value in udsa data for each year\n",
    "usda.groupby('year')['Value'].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the average value in udsa data for each year\n",
    "usda.groupby('year')['Value'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting the columns needed for analysis (year, county_name, county_ansi and Value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "usda['year'] = pd.to_datetime(usda['year'],\n",
    "               format='%Y')\n",
    "usda_subset = usda[['year','county_name','county_ansi','Value']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usda_subset['county_name'].nunique() #85 Michigan has 83"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is that the data set has 85 and Michigan only has 83. The additional two are 'OTHER COUNTIES' and 'OTHER (COMBINED) COUNTIES'.\n",
    "what do the entries look like for 'OTHER (COMBINED) COUNTIES'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usda_subset_other_combined = usda_subset[usda_subset['county_name'] == 'OTHER (COMBINED) COUNTIES']\n",
    "usda_subset_other_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 144 records that contain 'OTHER (COMBINED) COUNTIES' and three entries for 1960 and several entries for 2019. It appears that perhaps the \"Other Counties\" was a dumping ground for smaller quanities collected over several counties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 3 entries for 1960 have the following asd_desc: \"UPPER PENINSULA\", \"NORTHWEST\", and \"NORTHEAST\". The location_desc has these entries: \"MICHIGAN, UPPER PENINSULA, OTHER (COMBINED) COUNTIES\", \"MICHIGAN, NORTHWEST, OTHER (COMBINED) COUNTIES\", and \"MICHIGAN, NORTHEAST, OTHER (COMBINED) COUNTIES\". How many bushels/acre were reported total?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)\n",
    "usda_subset_1960 = usda[(usda['year'] == '1960-01-01') & usda['county_name'].str.upper().str.contains('OTHER')]\n",
    "usda_subset_1960"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usda_subset_1960.groupby('county_name')['Value'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total Value for 1960 \"OTHER (COMBINED) COUNTIES\" was 130.6\n",
    "A similiar dumping ground value when compared to the single \"OTHER COUNTIES\" in 2020.\n",
    "\n",
    "2019 seemed to have a lot of entries for \"OTHER (COMBINED) COUNTIES\". What is going on?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usda_2019 = usda[(usda['year'] == '2019-01-01') & (usda['county_name'] == 'OTHER (COMBINED) COUNTIES')]\n",
    "usda_2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 7 entries for 2019 have the following asd_desc: \"UPPER PENINSULA\", \"NORTHWEST\", \"NORTHEAST\",\"WEST CENTRAL\", \"CENTRAL\", \"SOUTHWEST\", and \"SOUTH CENTRAL\". The location_desc has these entries: \"MICHIGAN, UPPER PENINSULA, OTHER (COMBINED) COUNTIES\", \"MICHIGAN, NORTHWEST, OTHER (COMBINED) COUNTIES\", \"MICHIGAN, NORTHEAST, OTHER (COMBINED) COUNTIES\",\"MICHIGAN, WEST CENTRAL, OTHER (COMBINED) COUNTIES\", \"MICHIGAN, CENTRAL, OTHER (COMBINED) COUNTIES\", \"MICHIGAN, SOUTHWEST, OTHER (COMBINED) COUNTIES\", and \"MICHIGAN, SOUTH CENTRAL, OTHER (COMBINED) COUNTIES\".\n",
    "\n",
    "How many years and how many entries are we looking at that are like this? It looks like 2019 was the worst with 7 regions identified. The amount of bushels/acre is  886.3. This is considerably more than 1960."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = usda.groupby(['year', 'county_name']).agg(\n",
    "    count=('county_name', 'count'),\n",
    "    sum=('Value', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "filtered_df = grouped_df[grouped_df['count'] > 1]\n",
    "# Sort by 'sum' in descending order\n",
    "filtered_df = filtered_df.sort_values(by='sum', ascending=False)\n",
    "\n",
    "print(filtered_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When comparing to the rest of the entries in 2019. How bad does it look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "usda_subset_2019=usda_subset[usda_subset['year'] == '2019-01-01']\n",
    "grouped_df_2019 = usda_subset_2019.groupby(['year', 'county_name']).agg(\n",
    "    count=('county_name', 'count'),\n",
    "    sum=('Value', 'sum')\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust figure size for better readability\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Create the scatter plot\n",
    "g = sns.scatterplot(data=grouped_df_2019, x=\"county_name\", y=\"sum\", hue=\"year\")\n",
    "\n",
    "# Rotate the x-axis labels for readability\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Counties')\n",
    "plt.ylabel('BU/Acre')\n",
    "plt.title('Bushels/Acre by County for 2019')\n",
    "\n",
    "# Adjust the legend positioning\n",
    "#g.legend(loc='center left', bbox_to_anchor=(1.25, 0.5), ncol=1)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the counties for 2019 produced no more than 200 bushels/acre. As expected, \"OTHER (COMBINED) COUNTIES\" is the highest in the chart. What about those single entries for 'OTHER COUNTIES' after 2020?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usda_subset_other = usda_subset[usda_subset['county_name'] == 'OTHER COUNTIES']\n",
    "usda_subset_other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 4 records (years) that contain 'OTHER COUNTIES'; no duplicated years. We will go back to the original dataset and look at the entire record for those years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)\n",
    "usda_other = usda[usda['county_name'] == 'OTHER COUNTIES']\n",
    "usda_other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The asd_desc and location_desc is null for all 4 records is \"MICHIGAN, OTHER COUNTIES\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will take a look and see how many total counties are reporting beyond 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame for the year >= 2020\n",
    "usda_subset = usda[usda['year'] >= '2020']\n",
    "\n",
    "# Count the number of unique county names by year\n",
    "unique_counties_by_year = usda_subset.groupby('year')['county_name'].nunique()\n",
    "unique_counties_by_year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 61 counties in the data set for 2020. And those unique counties are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame for the year >= 2020\n",
    "usda_subset_2020 = usda_subset[usda_subset['year'] == '2020']\n",
    "usda_subset_2020['county_name'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "array(['DELTA', 'DICKINSON', 'MENOMINEE', 'ANTRIM', 'BENZIE',\n",
    "       'CHARLEVOIX', 'EMMET', 'GRAND TRAVERSE', 'LEELANAU', 'MANISTEE',\n",
    "       'WEXFORD', 'ALCONA', 'ALPENA', 'IOSCO', 'OGEMAW', 'OTSEGO',\n",
    "       'PRESQUE ISLE', 'MASON', 'MUSKEGON', 'NEWAYGO', 'OCEANA',\n",
    "       'GLADWIN', 'ISABELLA', 'MECOSTA', 'MIDLAND', 'MONTCALM', 'ARENAC',\n",
    "       'BAY', 'HURON', 'SAGINAW', 'SANILAC', 'TUSCOLA', 'ALLEGAN',\n",
    "       'BERRIEN', 'CASS', 'KALAMAZOO', 'KENT', 'OTTAWA', 'VAN BUREN',\n",
    "       'BARRY', 'BRANCH', 'CALHOUN', 'CLINTON', 'EATON', 'HILLSDALE',\n",
    "       'INGHAM', 'IONIA', 'JACKSON', 'ST JOSEPH', 'SHIAWASSEE', 'GENESEE',\n",
    "       'LAPEER', 'LENAWEE', 'LIVINGSTON', 'MACOMB', 'MONROE', 'OAKLAND',\n",
    "       'ST CLAIR', 'WASHTENAW', 'WAYNE', 'OTHER COUNTIES'], dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the average value in udsa data for each of those years\n",
    "mean_value = usda_subset.groupby('year')['Value'].mean()\n",
    "\n",
    "# create other Dataframe from orignial data\n",
    "usda_other = usda[usda['county_name'] == 'OTHER COUNTIES']\n",
    "usda_other\n",
    "\n",
    "# Merge the two DataFrames for comparison\n",
    "comparison = pd.merge(mean_value, usda_other, on='year', how='outer')\n",
    "\n",
    "# Rename columns for clarity\n",
    "comparison.rename(columns={'Value_x': 'Yearly Mean', 'Value_y':'Value'}, inplace=True)\n",
    "\n",
    "# Calculate the difference\n",
    "comparison['Difference'] = comparison['Yearly Mean'] - comparison['Value']\n",
    "comparison[['year','Yearly Mean', 'Value', 'Difference']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference is no greater than 14.7 bushels per acre. This is not significant when compared to the mean for each year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do the same for the \"OTHER (COMBINED)\" years. They stopped using that in 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame for the year <= 2019\n",
    "usda_subset_other_combined = usda[usda['year'] <= '2019']\n",
    "\n",
    "# what is the average value in udsa data for each of those years\n",
    "mean_value = usda_subset_other_combined.groupby('year')['Value'].mean()\n",
    "\n",
    "# Filter for counties where 'county_name' contains 'OTHER (COMBINED)' (case insensitive)\n",
    "other_combined = usda_subset_other_combined[usda_subset_other_combined['county_name'].str.contains('OTHER \\(COMBINED\\)', case=False, regex=True)]\n",
    "\n",
    "# Sum the 'Value' for OTHER (COMBINED) COUNTIES\n",
    "other_combined_sum = other_combined.groupby('year')['Value'].sum().reset_index('year')\n",
    "\n",
    "# Merge the two DataFrames for comparison and drop the records where there weren't \"Other (Combined)\" Counties\n",
    "comparison = pd.merge(mean_value, other_combined_sum, on='year', how='outer').dropna()\n",
    "\n",
    "# Rename columns for clarity\n",
    "comparison.rename(columns={'Value_x': 'Yearly Mean', 'Value_y':'Other Total Value'}, inplace=True)\n",
    "\n",
    "# Calculate the difference\n",
    "comparison['Difference'] = comparison['Yearly Mean'] - comparison['Other Total Value']\n",
    "comparison[['year','Yearly Mean', 'Other Total Value', 'Difference']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the figure size\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot the yearly mean and total value for \"Other Combined\" counties\n",
    "sns.lineplot(x='year', y='Yearly Mean', data=comparison, label='Yearly Mean')\n",
    "sns.lineplot(x='year', y='Other Total Value', data=comparison, label='Other Total Value')\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Yearly Mean vs. Other Combined Counties Total Value')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average value across counties does not change much over time, remaining stable even as the data for \"Other Combined\" counties fluctuates. The Other total value has several sharp spikes, with rapid increases and decreases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_usda(usda_df, start_year, end_year, other='Yes'):\n",
    "    # We will focus the fields from the dataframe we are interested in\n",
    "    filtered_df = usda_df[['year','county_name','state_ansi','county_ansi','Value']]\n",
    "\n",
    "    # We will change the datatype of year to datetime because it fits the data and we can have access to date functions\n",
    "    filtered_df.loc[:,'year'] = pd.to_datetime(filtered_df['year'],\n",
    "               format='%Y')\n",
    "    \n",
    "    # We need to ensure the start_year is a year too.\n",
    "    start_year = pd.to_datetime(str(start_year), format='%Y')\n",
    "\n",
    "    if end_year is not None:\n",
    "        end_year = pd.to_datetime(str(end_year), format='%Y')\n",
    "    else: end_year = start_year\n",
    "\n",
    "    # Filter starting from the requested year\n",
    "    filtered_df = filtered_df[(filtered_df['year'] >= start_year) & (filtered_df['year'] <= end_year)]\n",
    "   \n",
    "    # Remove the \"Other\" type county entries. We convert to all caps to handle any inconsistencies\n",
    "    if other == 'Yes':\n",
    "        filtered_df = filtered_df[~filtered_df['county_name'].str.upper().str.contains('OTHER')]\n",
    "    \n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try out the new function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usda_subset = clean_usda(usda,1950,2024,)\n",
    "usda_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a cool looking graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Create the plot\n",
    "ax = sns.histplot(data=usda_subset, x=\"Value\", hue=\"county_name\", multiple=\"stack\", palette=\"tab20\")\n",
    "\n",
    "plt.xlabel('BU/Acre')\n",
    "plt.title('Bushels/Acre by County for Past 7 Decades')\n",
    "\n",
    "# Manually create the legend with the right colors from the palette\n",
    "# Get the palette used for the plot\n",
    "palette = sns.color_palette(\"tab20\", n_colors=usda_subset['county_name'].nunique())\n",
    "\n",
    "# Get unique counties and map them to the corresponding palette color\n",
    "handles = [mpatches.Patch(color=palette[i], label=county) \n",
    "           for i, county in enumerate(usda_subset['county_name'].unique())]\n",
    "\n",
    "# Add the legend outside the plot\n",
    "ax.legend(handles=handles, bbox_to_anchor=(1.05, 1), loc='upper left', \n",
    "          borderaxespad=0, ncol=2, fontsize='small')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GDD calculation function\n",
    "def calculate_gdd(df, base_temp=50, upper_temp=86):\n",
    "    \"\"\"\n",
    "    Calculate Growing Degree Days (GDD) for corn.\n",
    "    \"\"\"\n",
    "    df['TMAX'] = df['TMAX'].clip(lower=base_temp, upper=upper_temp)\n",
    "    df['TMIN'] = df['TMIN'].clip(lower=base_temp)\n",
    "    df['TAVG'] = (df['TMAX'] + df['TMIN']) / 2\n",
    "    df['GDD'] = df['TAVG'] - base_temp\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = calculate_gdd(weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare average yield for usda_data_1955 and usda_data_2015\n",
    "usda_1955 = clean_usda(usda,1955,None,)\n",
    "usda_2015 = clean_usda(usda,2015,None,)\n",
    "\n",
    "usda_1955['Value'].mean(), usda_2015['Value'].mean()\n",
    "\n",
    "# whats the std deviation of yield for those years\n",
    "usda_1955['Value'].std(), usda_2015['Value'].std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare total gdd for weather_data_1955 and weather_data_2015 for county_ansi=161\n",
    "weather_2014 = pd.read_csv('data/weather_data_2014.csv')\n",
    "weather_2023 = pd.read_csv('data/weather_data_2023.csv')\n",
    "\n",
    "weather_2014 = calculate_gdd(weather_2014)\n",
    "weather_2023 = calculate_gdd(weather_2023)\n",
    "\n",
    "weather_2014[weather_2014['county_ansi'] == 161]['GDD'].sum(), weather_2023[weather_2023['county_ansi'] == 161]['GDD'].sum()\n",
    "\n",
    "# Do this for all years between 1950 and 1959\n",
    "gdd = []\n",
    "for year in range(1950, 1960):\n",
    "    weather = pd.read_csv(f'data/weather_data_{year}.csv')\n",
    "    weather = calculate_gdd(weather)\n",
    "    gdd.append(weather[weather['county_ansi'] == 161]['GDD'].sum())\n",
    "\n",
    "gdd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all weather data into a single DataFrame\n",
    "weather_data = pd.concat([pd.read_csv(f'data/weather_data_{year}.csv') for year in range(1990, 2024)])\n",
    "# Calculate GDD for each day\n",
    "weather_data = calculate_gdd(weather_data)\n",
    "weather_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the analysis, a function is created that we can send a usda dataframe, provide the start and end dates, and determine whether or not we want to include \"other counties\". I will focus on year, county_name, county_ansi, and Value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'date' to datetime if it's not already\n",
    "weather_data['date'] = pd.to_datetime(weather_data['date'])\n",
    "\n",
    "# Extract year and month\n",
    "weather_data['year'] = weather_data['date'].dt.year\n",
    "weather_data['month'] = weather_data['date'].dt.month\n",
    "\n",
    "# Aggregate GDD by county and year\n",
    "gdd_annual = weather_data.groupby(['state_ansi', 'county_ansi', 'year'])['GDD'].sum().reset_index()\n",
    "gdd_annual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all USDA data into a single DataFrame usda_data\n",
    "usda_data = clean_usda(usda,1990,2015,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I added some lines to clean the data, most files are fine but I am looking for the 'YIELD' data and skipping any suppressed data\n",
    "\n",
    "# Filter for the relevant data (e.g., 'YIELD' in 'statisticcat_desc')\n",
    "corn_data = usda_data[usda_data['statisticcat_desc'] == 'YIELD']\n",
    "\n",
    "# Convert 'Value' to numeric, removing any commas or missing values\n",
    "corn_data['Value'] = corn_data['Value'].replace(',', '', regex=True)\n",
    "corn_data = corn_data[corn_data['Value'] != '(D)']  # Remove suppressed data\n",
    "corn_data['Value'] = pd.to_numeric(corn_data['Value'])\n",
    "\n",
    "# Select relevant columns\n",
    "corn_data = corn_data[['state_ansi', 'county_ansi', 'year', 'Value', 'county_name']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the GDD and corn yield data to make a single DataFrame\n",
    "\n",
    "# Merge on 'state_ansi', 'county_ansi', and 'year'\n",
    "merged_data = pd.merge(gdd_annual, corn_data, on=['state_ansi', 'county_ansi', 'year'])\n",
    "# Remove entries with zero or NaN yields\n",
    "merged_data = merged_data[merged_data['Value'] > 0]\n",
    "# Remove entries with zero or NaN yields\n",
    "merged_data = merged_data[merged_data['GDD'] > 0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump merged_data to csv to test\n",
    "#merged_data.to_csv('merged_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data['Value'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data['GDD'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize Data Distribution**\n",
    "-------------------------------\n",
    "Plot boxplots to visualize the distribution of GDD and corn yield."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with subplots for side-by-side comparison\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 7))\n",
    "\n",
    "# Plot for 'GDD'\n",
    "axs[0].boxplot(merged_data['GDD'], patch_artist=True,\n",
    "               boxprops=dict(facecolor='skyblue', color='blue'),\n",
    "               flierprops=dict(marker='o', markerfacecolor='red', markersize=8, linestyle='none'),\n",
    "               medianprops=dict(color='orange', linewidth=2))\n",
    "axs[0].set_title('Corn Growing Degree Days (GDD)', fontsize=14)\n",
    "axs[0].set_ylabel('GDD', fontsize=12)\n",
    "axs[0].set_xlabel('Corn Growing Degree Days (GDD)', fontsize=12)\n",
    "axs[0].grid(True)\n",
    "\n",
    "# Overlay data points\n",
    "axs[0].scatter(np.ones(len(merged_data['GDD'])), merged_data['GDD'], color='black', alpha=0.5)\n",
    "\n",
    "# Plot for Corn\n",
    "axs[1].boxplot(merged_data['Value'], patch_artist=True,\n",
    "               boxprops=dict(facecolor='lightgreen', color='green'),\n",
    "               flierprops=dict(marker='o', markerfacecolor='red', markersize=8, linestyle='none'),\n",
    "               medianprops=dict(color='orange', linewidth=2))\n",
    "axs[1].set_title('Corn Yield', fontsize=14)\n",
    "axs[1].set_ylabel('Corn Yield', fontsize=12)\n",
    "axs[1].set_xlabel('Corn Yield', fontsize=12)\n",
    "axs[1].grid(True)\n",
    "\n",
    "# Overlay data points\n",
    "axs[1].scatter(np.ones(len(merged_data['Value'])), merged_data['Value'], color='black', alpha=0.5)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove Outliers**\n",
    "-------------------\n",
    "Identify and remove outliers using the Interquartile Range (IQR) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate IQR for 'GDD' and 'Value' columns to identify outliers\n",
    "Q1_GDD = merged_data['GDD'].quantile(0.25)\n",
    "Q3_GDD = merged_data['GDD'].quantile(0.75)\n",
    "IQR_GDD = Q3_GDD - Q1_GDD\n",
    "\n",
    "Q1_Value = merged_data['Value'].quantile(0.25)\n",
    "Q3_Value = merged_data['Value'].quantile(0.75)\n",
    "IQR_Value = Q3_Value - Q1_Value\n",
    "\n",
    "# Define bounds for outliers in 'GDD' and 'Value'\n",
    "lower_bound_GDD = Q1_GDD - 1.5 * IQR_GDD\n",
    "upper_bound_GDD = Q3_GDD + 1.5 * IQR_GDD\n",
    "\n",
    "lower_bound_Value = Q1_Value - 1.5 * IQR_Value\n",
    "upper_bound_Value = Q3_Value + 1.5 * IQR_Value\n",
    "\n",
    "# Filter out the outliers\n",
    "merged_data = merged_data[\n",
    "    (merged_data['GDD'] >= lower_bound_GDD) & (merged_data['GDD'] <= upper_bound_GDD) &\n",
    "    (merged_data['Value'] >= lower_bound_Value) & (merged_data['Value'] <= upper_bound_Value)\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Re-examine the Data**\n",
    "-----------------------\n",
    "Check the statistical summary after removing outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data['Value'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data['GDD'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump merged_data to csv to test\n",
    "merged_data.to_csv('merged_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize Correlation Between GDD and Corn Yield**\n",
    "----------------------------------------------------\n",
    "Create a scatter plot to visualize the correlation, with a color gradient showing time progression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot with color gradient for time progression\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Scatter plot with color mapped to 'year' and a plasma colormap for better color representation\n",
    "scatter = plt.scatter(x=merged_data['GDD'], y=merged_data['Value'], c=merged_data['year'], cmap='plasma', s=10)\n",
    "\n",
    "# Add colorbar to show year progression\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_label('Year')\n",
    "\n",
    "# Set titles and labels\n",
    "plt.title('Correlation between GDD and Corn Yield with Time Progression', fontsize=14)\n",
    "plt.xlabel('Growing Degree Days (GDD)', fontsize=12)\n",
    "plt.ylabel('Corn Yield (bu/acre)', fontsize=12)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of GDD vs. corn yield\n",
    "sns.lmplot(x='GDD', y='Value', data=merged_data, scatter_kws={'s': 1})\n",
    "plt.title('Correlation between GDD and Corn Yield')\n",
    "plt.xlabel('Growing Degree Days (GDD)')\n",
    "plt.ylabel('Corn Yield (bu/acre)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyze Precipitation Data**\n",
    "------------------------------\n",
    "We will analyze the correlation between total precipitation (PRCP) and corn yield."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the same chart with PRCP instead of GDD\n",
    "\n",
    "# Calculate total precipitation for the growing season\n",
    "weather_data['PRCP'] = weather_data['PRCP'].clip(lower=0)\n",
    "prcp_annual = weather_data.groupby(['state_ansi', 'county_ansi', 'year'])['PRCP'].sum().reset_index()\n",
    "\n",
    "# Merge on 'state_ansi', 'county_ansi', and 'year'\n",
    "merged_data_prcp = pd.merge(prcp_annual, corn_data, on=['state_ansi', 'county_ansi', 'year'])\n",
    "# Remove entries with zero or NaN yields\n",
    "merged_data_prcp = merged_data_prcp[merged_data_prcp['Value'] > 0]\n",
    "# Remove entries with zero or NaN yields\n",
    "merged_data_prcp = merged_data_prcp[merged_data_prcp['PRCP'] > 0]\n",
    "\n",
    "# Scatter plot with regression line\n",
    "sns.lmplot(x='PRCP', y='Value', data=merged_data_prcp, scatter_kws={'s': 0.5})\n",
    "plt.title('Correlation between PRCP and Corn Yield')\n",
    "plt.xlabel('Total Precipitation (mm)')\n",
    "plt.ylabel('Corn Yield (bu/acre)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the data was similar to the GDD data, we dind't feel it was necessary to include the analysis of the precipitation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add precipitation data to the merged dataframe\n",
    "#prcp_annual = weather_data.groupby(['state_ansi', 'county_ansi', 'year'])['PRCP'].sum().reset_index()\n",
    "#merged_data = pd.merge(merged_data, prcp_annual, on=['state_ansi', 'county_ansi', 'year'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perform Linear Regression Analysis**\n",
    "--------------------------------------\n",
    "We will perform an Ordinary Least Squares (OLS) regression to quantify the relationship between GDD and corn yield."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Prepare the data for OLS (adding a constant for intercept)\n",
    "X = sm.add_constant(merged_data['GDD'])  # Independent variable (GDD) with a constant\n",
    "y = merged_data['Value']  # Dependent variable (Corn Yield)\n",
    "\n",
    "# Step 2: Fit the OLS model\n",
    "ols_model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Step 3: Print out the summary of the regression\n",
    "print(ols_model.summary())\n",
    "\n",
    "# Step 4: Plot the scatter plot with regression line (optional, already in your code)\n",
    "sns.lmplot(x='GDD', y='Value', data=merged_data, scatter_kws={'s': 1})\n",
    "plt.title('Correlation between GDD and Corn Yield')\n",
    "plt.xlabel('Growing Degree Days (GDD)')\n",
    "plt.ylabel('Corn Yield (bu/acre)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multiple Linear Regression**\n",
    "------------------------------\n",
    "Include both GDD and Year as independent variables to see if time has an effect on yield."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Prepare the data for OLS (adding a constant for intercept)\n",
    "X = merged_data[['GDD', 'year']]  # Independent variables (GDD and Year)\n",
    "X = sm.add_constant(X)  # Add constant for the intercept\n",
    "y = merged_data['Value']  # Dependent variable (Corn Yield)\n",
    "\n",
    "# Step 2: Fit the OLS model\n",
    "ols_model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Step 3: Print out the summary of the regression\n",
    "print(ols_model.summary())\n",
    "\n",
    "# Step 4: Plot the scatter plot with regression line for GDD (optional)\n",
    "sns.lmplot(x='GDD', y='Value', data=merged_data, scatter_kws={'s': 1})\n",
    "plt.title('Correlation between GDD and Corn Yield')\n",
    "plt.xlabel('Growing Degree Days (GDD)')\n",
    "plt.ylabel('Corn Yield (bu/acre)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define independent variables and dependent variable\n",
    "#X = merged_data[['GDD', 'PRCP']]\n",
    "X = merged_data[['Value']]\n",
    "y = merged_data['GDD']\n",
    "\n",
    "# Add a constant term for the intercept\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the regression model\n",
    "model = sm.OLS(y, X).fit()\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ranking**\n",
    "------------\n",
    "We determined a ranking of the counties based on a number of factors and will use a normalized score to rank the counties. in the end.\n",
    "\n",
    "- Highest Average Yield\n",
    "- Consistency Above State Average\n",
    "- Highest GDD\n",
    "- Low Variability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average yield per county over the entire period\n",
    "average_yield = merged_data.groupby('county_name')['Value'].mean().reset_index()\n",
    "\n",
    "# Sort counties by average yield in descending order\n",
    "average_yield_sorted = average_yield.sort_values(by='Value', ascending=False)\n",
    "\n",
    "# Top 10 counties\n",
    "print(\"Top 10 Counties by Average Corn Yield:\")\n",
    "print(average_yield_sorted.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizontal bar plot of average yields for top 10 counties in reverse order\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.barh(average_yield_sorted['county_name'].head(10)[::-1], average_yield_sorted['Value'].head(10)[::-1], color='skyblue')\n",
    "plt.ylabel('County')\n",
    "plt.xlabel('Average Corn Yield (bu/acre)')\n",
    "plt.title('Top 10 Counties by Average Corn Yield')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I was wondering if we pick the best based how consistent are they in beating the state average\n",
    "\n",
    "# Calculate the state average yield for each year\n",
    "state_average_yield = merged_data.groupby('year')['Value'].mean().reset_index(name='state_avg_yield')\n",
    "\n",
    "# Merge state average back into merged_data\n",
    "merged_data_with_state_avg = pd.merge(merged_data, state_average_yield, on='year')\n",
    "\n",
    "# Create a flag for whether the county's yield is above the state average each year\n",
    "merged_data_with_state_avg['above_state_avg'] = merged_data_with_state_avg['Value'] > merged_data_with_state_avg['state_avg_yield']\n",
    "\n",
    "# Calculate the percentage of years each county was above the state average\n",
    "county_performance = merged_data_with_state_avg.groupby('county_name')['above_state_avg'].mean().reset_index()\n",
    "\n",
    "# Convert to percentage\n",
    "county_performance['percent_above_state_avg'] = county_performance['above_state_avg'] * 100\n",
    "\n",
    "# Sort counties by percentage in descending order\n",
    "county_performance_sorted = county_performance.sort_values(by='percent_above_state_avg', ascending=False)\n",
    "\n",
    "# Display the top 10 counties\n",
    "print(\"Top 10 Counties by Consistency in Exceeding State Average Yield:\")\n",
    "print(county_performance_sorted.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizontal bar plot of top 10 counties by percentage of years above state average in reverse order\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.barh(county_performance_sorted['county_name'].head(10)[::-1], county_performance_sorted['percent_above_state_avg'].head(10)[::-1], color='orange')\n",
    "plt.ylabel('County')\n",
    "plt.xlabel('Percentage of Years Above State Average (%)')\n",
    "plt.title('Top 10 Counties Consistently Exceeding State Average Yield')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about finding the best improved county over time wiht slope\n",
    "\n",
    "# Prepare a DataFrame to store slopes\n",
    "county_slopes = []\n",
    "\n",
    "for county in merged_data['county_name'].unique():\n",
    "    county_data = merged_data[merged_data['county_name'] == county]\n",
    "    if len(county_data['year'].unique()) > 1:  # Ensure there's enough data\n",
    "        slope, intercept, r_value, p_value, std_err = linregress(county_data['year'], county_data['Value'])\n",
    "        county_slopes.append({'county_name': county, 'slope': slope, 'p_value': p_value})\n",
    "    else:\n",
    "        county_slopes.append({'county_name': county, 'slope': np.nan, 'p_value': np.nan})\n",
    "\n",
    "# Convert to DataFrame\n",
    "slopes_df = pd.DataFrame(county_slopes)\n",
    "\n",
    "# Remove counties with NaN slopes\n",
    "slopes_df.dropna(subset=['slope'], inplace=True)\n",
    "\n",
    "# Sort counties by slope\n",
    "slopes_df_sorted = slopes_df.sort_values(by='slope', ascending=False)\n",
    "\n",
    "print(\"Top 10 Counties by Yield Improvement Over Time:\")\n",
    "print(slopes_df_sorted.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizontal bar plot of top 10 counties by yield improvement over time (slope)\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.barh(slopes_df_sorted['county_name'].head(10)[::-1], slopes_df_sorted['slope'].head(10)[::-1], color='green')\n",
    "plt.ylabel('County')\n",
    "plt.xlabel('Slope of Yield Improvement Over Time')\n",
    "plt.title('Top 10 Counties by Yield Improvement Over Time')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets looks at the counties that have been the most consistent in their yield over time\n",
    "\n",
    "# Calculate standard deviation of yield per county\n",
    "yield_variability = merged_data.groupby('county_name')['Value'].std().reset_index(name='yield_std')\n",
    "\n",
    "# Sort counties by yield_std in ascending order\n",
    "yield_variability_sorted = yield_variability.sort_values(by='yield_std')\n",
    "\n",
    "print(\"Top 10 Counties with Least Yield Variability:\")\n",
    "print(yield_variability_sorted.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizontal bar plot of top 10 counties with least yield variability (standard deviation)\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.barh(yield_variability_sorted['county_name'].head(10)[::-1], yield_variability_sorted['yield_std'].head(10)[::-1], color='purple')\n",
    "plt.ylabel('County')\n",
    "plt.xlabel('Yield Variability (Standard Deviation)')\n",
    "plt.title('Top 10 Counties with Least Yield Variability (Reversed Order)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look at the counties with the highest average gdd\n",
    "\n",
    "# Calculate average GDD per county\n",
    "avg_gdd_per_county = merged_data.groupby('county_name')['GDD'].mean().reset_index()\n",
    "\n",
    "# Sort counties by average GDD in descending order\n",
    "avg_gdd_per_county_sorted = avg_gdd_per_county.sort_values(by='GDD', ascending=False)\n",
    "\n",
    "print(\"Top 10 Counties by Average Growing Degree Days:\")\n",
    "print(avg_gdd_per_county_sorted.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizontal bar plot of top 10 counties by average GDD\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.barh(avg_gdd_per_county_sorted['county_name'].head(10)[::-1], avg_gdd_per_county_sorted['GDD'].head(10)[::-1], color='red')\n",
    "plt.ylabel('County')\n",
    "plt.xlabel('Average Growing Degree Days (GDD)')\n",
    "plt.title('Top 10 Counties by Average Growing Degree Days')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use county_name to merge all the other DFs\n",
    "county_metrics = average_yield[['county_name', 'Value']].merge(\n",
    "    county_performance[['county_name', 'percent_above_state_avg']], on='county_name').merge(\n",
    "    yield_variability[['county_name', 'yield_std']], on='county_name').merge(\n",
    "    avg_gdd_per_county[['county_name', 'GDD']], on='county_name')\n",
    "\n",
    "# Normalize the metrics\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "county_metrics[['avg_yield_norm', 'percent_above_avg_norm', 'yield_std_norm', 'avg_gdd_per_county']] = scaler.fit_transform(\n",
    "    county_metrics[['Value', 'percent_above_state_avg', 'yield_std', 'GDD']])\n",
    "\n",
    "# Invert yield_std_norm so that lower variability scores higher\n",
    "county_metrics['yield_std_norm'] = 1 - county_metrics['yield_std_norm']\n",
    "\n",
    "# I assigned weights to each metric but nothing scientific\n",
    "# Should come back to this later\n",
    "county_metrics['total_score'] = (\n",
    "    county_metrics['avg_yield_norm'] * 0.4 +\n",
    "    county_metrics['percent_above_avg_norm'] * 0.3 +\n",
    "    county_metrics['yield_std_norm'] * 0.1 +\n",
    "    county_metrics['avg_gdd_per_county'] * 0.1\n",
    ")\n",
    "\n",
    "# Sort counties by total_score\n",
    "county_metrics_sorted = county_metrics.sort_values(by='total_score', ascending=False)\n",
    "\n",
    "print(\"Top 10 Counties by Composite Score:\")\n",
    "print(county_metrics_sorted[['county_name', 'total_score']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot of top 10 counties by composite score\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(county_metrics_sorted['county_name'].head(10), county_metrics_sorted['total_score'].head(10), color='purple')\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('County')\n",
    "plt.ylabel('Composite Score')\n",
    "plt.title('Top 10 Counties by Composite Score')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for heatmap\n",
    "metrics_for_heatmap = county_metrics_sorted[['county_name', 'avg_yield_norm', 'percent_above_avg_norm', 'yield_std_norm', 'avg_gdd_per_county', 'total_score']].set_index('county_name')\n",
    "\n",
    "# Rename the columns to make the metric names more readable\n",
    "metrics_for_heatmap.rename(columns={\n",
    "    'avg_yield_norm': 'Normalized Avg Yield',\n",
    "    'percent_above_avg_norm': 'Percent Years Above Avg Yield',\n",
    "    'yield_std_norm': 'Normalized Yield Variability',\n",
    "    'avg_gdd_per_county': 'Avg Growing Degree Days (GDD)',\n",
    "    'total_score': 'Total Score'\n",
    "}, inplace=True)\n",
    "\n",
    "# Set a larger and professional style\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plot heatmap with enhancements\n",
    "heatmap = sns.heatmap(metrics_for_heatmap.head(10), \n",
    "                      annot=True, \n",
    "                      cmap='coolwarm',  # More polished colormap\n",
    "                      linewidths=0.5,   # Add gridlines\n",
    "                      linecolor='black', # Gridline color\n",
    "                      annot_kws={\"size\": 10},  # Annotation text size\n",
    "                      cbar_kws={'label': 'Normalized Values'})  # Label the color bar\n",
    "\n",
    "# Enhance title and axis labels\n",
    "plt.title('Normalized Metrics and Total Score for Top 10 Counties', fontsize=16, weight='bold', pad=20)\n",
    "plt.xlabel('Metrics', fontsize=12)\n",
    "plt.ylabel('County', fontsize=12)\n",
    "\n",
    "# Add a descriptive legend for x-axis values\n",
    "plt.figtext(0.46, -0.1,\n",
    "            'Normalized Avg Yield: Normalized average corn yield per county\\n'\n",
    "            'Percent Years Above Avg Yield: Percent of years the county yield exceeded the state average\\n'\n",
    "            'Normalized Yield Variability: Lower values indicate more consistent yields\\n'\n",
    "            'Avg Growing Degree Days (GDD): Average GDD needed for corn production\\n'\n",
    "            'Total Score: Composite score from the weighted sum of all metrics',\n",
    "            ha='center', fontsize=10, wrap=True, bbox={\"facecolor\":\"lightgray\", \"alpha\":0.5, \"pad\":5})\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.xticks(rotation=30, ha='right', fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the heatmap\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
