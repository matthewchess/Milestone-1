{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.stats import linregress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all the data/usda_* into one dataframe\n",
    "usda = pd.concat([pd.read_csv(f'data/usda_data_{year}.csv')  for year in range(1950, 2024)])\n",
    "\n",
    "# combine all the data/weather_* into one dataframe\n",
    "weather = pd.concat([pd.read_csv(f'data/weather_data_{year}.csv') for year in range(2000, 2021)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usda.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the average value in udsa data for each year\n",
    "usda.groupby('year')['Value'].mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll select the columns that I need for analysis. I will focus on year, county_name and Value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the average value in udsa data for each year\n",
    "usda.groupby('year')['Value'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "usda['year'] = pd.to_datetime(usda['year'],\n",
    "               format='%Y')\n",
    "usda_subset = usda[['year','county_name','Value']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usda_subset['county_name'].nunique() #85 Michigan has 83"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is that the data set has 85 and Michigan only has 83. The additional two are 'OTHER COUNTIES' and 'OTHER (COMBINED) COUNTIES'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usda_subset_other_combined = usda_subset[usda_subset['county_name'] == 'OTHER (COMBINED) COUNTIES']\n",
    "usda_subset_other_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 144 records that contain 'OTHER (COMBINED) COUNTIES'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usda_subset_other = usda_subset[usda_subset['county_name'] == 'OTHER COUNTIES']\n",
    "usda_subset_other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\tyear\tcounty_name\tValue\n",
    "60\t2020-01-01\tOTHER COUNTIES\t151.0\n",
    "57\t2021-01-01\tOTHER COUNTIES\t150.2\n",
    "59\t2022-01-01\tOTHER COUNTIES\t163.2\n",
    "57\t2023-01-01\tOTHER COUNTIES\t166.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 4 records (years) that contain 'OTHER COUNTIES'. We will focus on the year 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usda_other = usda[usda['county_name'] == 'OTHER COUNTIES']\n",
    "usda_other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The asd_desc is null for all 4 records. location_desc for all 4 records is \"MICHIGAN, OTHER COUNTIES\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "usda_subset_2020=usda_subset[usda_subset['year'] == '2020-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usda_subset_2020['county_name'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 61 counties in the data set for 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usda_subset_2020['county_name'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "array(['DELTA', 'DICKINSON', 'MENOMINEE', 'ANTRIM', 'BENZIE',\n",
    "       'CHARLEVOIX', 'EMMET', 'GRAND TRAVERSE', 'LEELANAU', 'MANISTEE',\n",
    "       'WEXFORD', 'ALCONA', 'ALPENA', 'IOSCO', 'OGEMAW', 'OTSEGO',\n",
    "       'PRESQUE ISLE', 'MASON', 'MUSKEGON', 'NEWAYGO', 'OCEANA',\n",
    "       'GLADWIN', 'ISABELLA', 'MECOSTA', 'MIDLAND', 'MONTCALM', 'ARENAC',\n",
    "       'BAY', 'HURON', 'SAGINAW', 'SANILAC', 'TUSCOLA', 'ALLEGAN',\n",
    "       'BERRIEN', 'CASS', 'KALAMAZOO', 'KENT', 'OTTAWA', 'VAN BUREN',\n",
    "       'BARRY', 'BRANCH', 'CALHOUN', 'CLINTON', 'EATON', 'HILLSDALE',\n",
    "       'INGHAM', 'IONIA', 'JACKSON', 'ST JOSEPH', 'SHIAWASSEE', 'GENESEE',\n",
    "       'LAPEER', 'LENAWEE', 'LIVINGSTON', 'MACOMB', 'MONROE', 'OAKLAND',\n",
    "       'ST CLAIR', 'WASHTENAW', 'WAYNE', 'OTHER COUNTIES'], dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=usda_subset, x=\"Value\", hue=\"county_name\", legend=False)\n",
    "plt.xlabel('BU/Acre')\n",
    "plt.title('Bushels/Acre by County for Past 2 Decades')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust figure size for better readability\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Create the scatter plot\n",
    "g = sns.scatterplot(data=usda_subset_2020, x=\"county_name\", y=\"Value\", hue=\"year\")\n",
    "\n",
    "# Rotate the x-axis labels for readability\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Counties')\n",
    "plt.ylabel('BU/Acre')\n",
    "plt.title('Bushels/Acre by County for 2020')\n",
    "\n",
    "# Adjust the legend positioning\n",
    "#g.legend(loc='center left', bbox_to_anchor=(1.25, 0.5), ncol=1)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that perhaps the \"Other Counties\" was a dumping ground for smaller quanities collected over several counties. There is only one dot respresenting the one entry for the year 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was there really 3 entries for 1960 for \"Other (Combined) Counties\"? A dataset containing the 1960 entries will be created and investigated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "usda_subset_1960=usda_subset[usda_subset['year'] == '1960-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust figure size for better readability\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Create the scatter plot\n",
    "g = sns.scatterplot(data=usda_subset_1960, x=\"county_name\", y=\"Value\", hue=\"year\")\n",
    "\n",
    "# Rotate the x-axis labels for readability\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Counties')\n",
    "plt.ylabel('BU/Acre')\n",
    "plt.title('Bushels/Acre by County for 1960')\n",
    "\n",
    "# Adjust the legend positioning\n",
    "#g.legend(loc='center left', bbox_to_anchor=(1.25, 0.5), ncol=1)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure enough there are three dots over the \"Other (Combined) Counties\" mark. What does it look like when we look at the entered values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usda_1960 = usda[(usda['year'] == '1960-01-01') & (usda['county_name'] == 'OTHER (COMBINED) COUNTIES')]\n",
    "usda_1960"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 3 entries for 1960 have the following asd_desc: \"UPPER PENINSULA\", \"NORTHWEST\", and \"NORTHEAST\". The location_desc has these entries: \"MICHIGAN, UPPER PENINSULA, OTHER (COMBINED) COUNTIES\", \"MICHIGAN, NORTHWEST, OTHER (COMBINED) COUNTIES\", and \"MICHIGAN, NORTHEAST, OTHER (COMBINED) COUNTIES\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = usda_subset_1960.groupby('county_name')['Value'].sum().reset_index()\n",
    "print(grouped_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total Value for 1960 \"OTHER (COMBINED) COUNTIES\" was 130.6\n",
    "A similiar dumping ground value when compared to the single \"OTHER COUNTIES\" in 2020.\n",
    "\n",
    "2019 seemed to have a lot of entries for \"OTHER (COMBINED) COUNTIES\". What is going on?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usda_2019 = usda[(usda['year'] == '2019-01-01') & (usda['county_name'] == 'OTHER (COMBINED) COUNTIES')]\n",
    "usda_2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 7 entries for 2019 have the following asd_desc: \"UPPER PENINSULA\", \"NORTHWEST\", \"NORTHEAST\",\"WEST CENTRAL\", \"CENTRAL\", \"SOUTHWEST\", and \"SOUTH CENTRAL\". The location_desc has these entries: \"MICHIGAN, UPPER PENINSULA, OTHER (COMBINED) COUNTIES\", \"MICHIGAN, NORTHWEST, OTHER (COMBINED) COUNTIES\", \"MICHIGAN, NORTHEAST, OTHER (COMBINED) COUNTIES\",\"MICHIGAN, WEST CENTRAL, OTHER (COMBINED) COUNTIES\", \"MICHIGAN, CENTRAL, OTHER (COMBINED) COUNTIES\", \"MICHIGAN, SOUTHWEST, OTHER (COMBINED) COUNTIES\", and \"MICHIGAN, SOUTH CENTRAL, OTHER (COMBINED) COUNTIES\".\n",
    "\n",
    "How many years and how many entries are we looking at that are like this? It looks like 2019 was the worst with 7 regions identified. The amount of bushels/acre is  886.3. This is considerably more than 1960."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = usda.groupby(['year', 'county_name']).agg(\n",
    "    count=('county_name', 'count'),\n",
    "    sum=('Value', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "filtered_df = grouped_df[grouped_df['count'] > 1]\n",
    "# Sort by 'sum' in descending order\n",
    "filtered_df = filtered_df.sort_values(by='sum', ascending=False)\n",
    "\n",
    "print(filtered_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "             year                county_name  count    sum\n",
    "4768 2019-01-01  OTHER (COMBINED) COUNTIES      7  886.3\n",
    "4610 2016-01-01  OTHER (COMBINED) COUNTIES      6  824.1\n",
    "4551 2015-01-01  OTHER (COMBINED) COUNTIES      6  823.4\n",
    "4670 2017-01-01  OTHER (COMBINED) COUNTIES      5  655.8\n",
    "3942 2005-01-01  OTHER (COMBINED) COUNTIES      5  576.0\n",
    "3638 2000-01-01  OTHER (COMBINED) COUNTIES      6  565.0\n",
    "4001 2006-01-01  OTHER (COMBINED) COUNTIES      5  555.0\n",
    "3589 1999-01-01  OTHER (COMBINED) COUNTIES      5  533.0\n",
    "4061 2007-01-01  OTHER (COMBINED) COUNTIES      6  502.0\n",
    "3881 2004-01-01  OTHER (COMBINED) COUNTIES      5  453.0\n",
    "4120 2008-01-01  OTHER (COMBINED) COUNTIES      5  442.0\n",
    "4493 2014-01-01  OTHER (COMBINED) COUNTIES      4  436.4\n",
    "3820 2003-01-01  OTHER (COMBINED) COUNTIES      4  398.0\n",
    "4717 2018-01-01  OTHER (COMBINED) COUNTIES      3  397.1\n",
    "4248 2010-01-01  OTHER (COMBINED) COUNTIES      3  385.7\n",
    "3464 1997-01-01  OTHER (COMBINED) COUNTIES      4  345.0\n",
    "3691 2001-01-01  OTHER (COMBINED) COUNTIES      6  327.0\n",
    "3757 2002-01-01  OTHER (COMBINED) COUNTIES      3  308.0\n",
    "4183 2009-01-01  OTHER (COMBINED) COUNTIES      3  303.0\n",
    "4314 2011-01-01  OTHER (COMBINED) COUNTIES      3  288.7\n",
    "4432 2013-01-01  OTHER (COMBINED) COUNTIES      2  245.4\n",
    "4377 2012-01-01  OTHER (COMBINED) COUNTIES      3  239.2\n",
    "3530 1998-01-01  OTHER (COMBINED) COUNTIES      3  230.0\n",
    "3406 1996-01-01  OTHER (COMBINED) COUNTIES      3  206.0\n",
    "3340 1995-01-01  OTHER (COMBINED) COUNTIES      2  180.0\n",
    "1232 1967-01-01  OTHER (COMBINED) COUNTIES      3  172.8\n",
    "2190 1980-01-01  OTHER (COMBINED) COUNTIES      2  160.6\n",
    "1128 1965-01-01  OTHER (COMBINED) COUNTIES      3  159.3\n",
    "1180 1966-01-01  OTHER (COMBINED) COUNTIES      3  152.5\n",
    "2044 1978-01-01  OTHER (COMBINED) COUNTIES      2  150.0\n",
    "2117 1979-01-01  OTHER (COMBINED) COUNTIES      2  147.4\n",
    "952  1962-01-01  OTHER (COMBINED) COUNTIES      3  144.8\n",
    "1004 1963-01-01  OTHER (COMBINED) COUNTIES      3  142.2\n",
    "900  1961-01-01  OTHER (COMBINED) COUNTIES      3  139.2\n",
    "848  1960-01-01  OTHER (COMBINED) COUNTIES      3  130.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When comparing to the rest of the entries in 2019. How bad does it look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "usda_subset_2019=usda_subset[usda_subset['year'] == '2019-01-01']\n",
    "grouped_df_2019 = usda_subset_2019.groupby(['year', 'county_name']).agg(\n",
    "    count=('county_name', 'count'),\n",
    "    sum=('Value', 'sum')\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust figure size for better readability\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Create the scatter plot\n",
    "g = sns.scatterplot(data=grouped_df_2019, x=\"county_name\", y=\"sum\", hue=\"year\")\n",
    "\n",
    "# Rotate the x-axis labels for readability\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Counties')\n",
    "plt.ylabel('BU/Acre')\n",
    "plt.title('Bushels/Acre by County for 2019')\n",
    "\n",
    "# Adjust the legend positioning\n",
    "#g.legend(loc='center left', bbox_to_anchor=(1.25, 0.5), ncol=1)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the counties for 2019 produced no more than 200 bushels/acre. As expected, \"OTHER (COMBINED) COUNTIES\" is the highest in the chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GDD calculation function\n",
    "def calculate_gdd(df, base_temp=50, upper_temp=86):\n",
    "    \"\"\"\n",
    "    Calculate Growing Degree Days (GDD) for corn.\n",
    "    \"\"\"\n",
    "    df['TMAX'] = df['TMAX'].clip(lower=base_temp, upper=upper_temp)\n",
    "    df['TMIN'] = df['TMIN'].clip(lower=base_temp)\n",
    "    df['TAVG'] = (df['TMAX'] + df['TMIN']) / 2\n",
    "    df['GDD'] = df['TAVG'] - base_temp\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = calculate_gdd(weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare average yield for usda_data_1955 and usda_data_2015\n",
    "usda_1955 = pd.read_csv('data/usda_data_1955.csv')\n",
    "usda_2015 = pd.read_csv('data/usda_data_2015.csv')\n",
    "\n",
    "usda_1955['Value'].mean(), usda_2015['Value'].mean()\n",
    "\n",
    "# whats the std deviation of yield for those years\n",
    "usda_1955['Value'].std(), usda_2015['Value'].std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare total gdd for weather_data_1955 and weather_data_2015 for county_ansi=161\n",
    "weather_2014 = pd.read_csv('data/weather_data_2014.csv')\n",
    "weather_2023 = pd.read_csv('data/weather_data_2023.csv')\n",
    "\n",
    "weather_2014 = calculate_gdd(weather_2014)\n",
    "weather_2023 = calculate_gdd(weather_2023)\n",
    "\n",
    "weather_2014[weather_2014['county_ansi'] == 161]['GDD'].sum(), weather_2023[weather_2023['county_ansi'] == 161]['GDD'].sum()\n",
    "\n",
    "# Do this for all years between 1950 and 1959\n",
    "gdd = []\n",
    "for year in range(1950, 1960):\n",
    "    weather = pd.read_csv(f'data/weather_data_{year}.csv')\n",
    "    weather = calculate_gdd(weather)\n",
    "    gdd.append(weather[weather['county_ansi'] == 161]['GDD'].sum())\n",
    "\n",
    "gdd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all weather data into a single DataFrame\n",
    "weather_data = pd.concat([pd.read_csv(f'data/weather_data_{year}.csv') for year in range(1990, 2024)])\n",
    "# Calculate GDD for each day\n",
    "weather_data = calculate_gdd(weather_data)\n",
    "weather_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'date' to datetime if it's not already\n",
    "weather_data['date'] = pd.to_datetime(weather_data['date'])\n",
    "\n",
    "# Extract year and month\n",
    "weather_data['year'] = weather_data['date'].dt.year\n",
    "weather_data['month'] = weather_data['date'].dt.month\n",
    "\n",
    "# Aggregate GDD by county and year\n",
    "gdd_annual = weather_data.groupby(['state_ansi', 'county_ansi', 'year'])['GDD'].sum().reset_index()\n",
    "gdd_annual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all USDA data into a single DataFrame usda_data\n",
    "usda_data = pd.concat([pd.read_csv(f'data/usda_data_{year}.csv') for year in range(1990, 2024)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I added some lines to clean the data, most files are fine but I am looking for the 'YIELD' data and skipping any suppressed data\n",
    "\n",
    "# Filter for the relevant data (e.g., 'YIELD' in 'statisticcat_desc')\n",
    "corn_data = usda_data[usda_data['statisticcat_desc'] == 'YIELD']\n",
    "\n",
    "# Convert 'Value' to numeric, removing any commas or missing values\n",
    "corn_data['Value'] = corn_data['Value'].replace(',', '', regex=True)\n",
    "corn_data = corn_data[corn_data['Value'] != '(D)']  # Remove suppressed data\n",
    "corn_data['Value'] = pd.to_numeric(corn_data['Value'])\n",
    "\n",
    "# Select relevant columns\n",
    "corn_data = corn_data[['state_ansi', 'county_ansi', 'year', 'Value', 'county_name']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the GDD and corn yield data to make a single DataFrame\n",
    "\n",
    "# Merge on 'state_ansi', 'county_ansi', and 'year'\n",
    "merged_data = pd.merge(gdd_annual, corn_data, on=['state_ansi', 'county_ansi', 'year'])\n",
    "# Remove entries with zero or NaN yields\n",
    "merged_data = merged_data[merged_data['Value'] > 0]\n",
    "# Remove entries with zero or NaN yields\n",
    "merged_data = merged_data[merged_data['GDD'] > 0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump merged_data to csv to test\n",
    "#merged_data.to_csv('merged_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data['Value'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data['GDD'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize Data Distribution**\n",
    "-------------------------------\n",
    "Plot boxplots to visualize the distribution of GDD and corn yield."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for 'GDD'\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.boxplot(merged_data['GDD'], tick_labels=['']) \n",
    "plt.title('Corn Growing Degree Days (GDD)', fontsize=14)\n",
    "plt.ylabel('GDD', fontsize=12)\n",
    "plt.xlabel('Corn Growing Degree Days (GDD)', fontsize=12)\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot for 'Value'\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.boxplot(merged_data['Value'], tick_labels=[''])\n",
    "plt.title('Corn Yield', fontsize=14)\n",
    "plt.ylabel('Corn Yield', fontsize=12)\n",
    "plt.xlabel('Corn Yield', fontsize=12)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove Outliers**\n",
    "-------------------\n",
    "Identify and remove outliers using the Interquartile Range (IQR) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate IQR for 'GDD' and 'Value' columns to identify outliers\n",
    "Q1_GDD = merged_data['GDD'].quantile(0.25)\n",
    "Q3_GDD = merged_data['GDD'].quantile(0.75)\n",
    "IQR_GDD = Q3_GDD - Q1_GDD\n",
    "\n",
    "Q1_Value = merged_data['Value'].quantile(0.25)\n",
    "Q3_Value = merged_data['Value'].quantile(0.75)\n",
    "IQR_Value = Q3_Value - Q1_Value\n",
    "\n",
    "# Define bounds for outliers in 'GDD' and 'Value'\n",
    "lower_bound_GDD = Q1_GDD - 1.5 * IQR_GDD\n",
    "upper_bound_GDD = Q3_GDD + 1.5 * IQR_GDD\n",
    "\n",
    "lower_bound_Value = Q1_Value - 1.5 * IQR_Value\n",
    "upper_bound_Value = Q3_Value + 1.5 * IQR_Value\n",
    "\n",
    "# Filter out the outliers\n",
    "merged_data = merged_data[\n",
    "    (merged_data['GDD'] >= lower_bound_GDD) & (merged_data['GDD'] <= upper_bound_GDD) &\n",
    "    (merged_data['Value'] >= lower_bound_Value) & (merged_data['Value'] <= upper_bound_Value)\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Re-examine the Data**\n",
    "-----------------------\n",
    "Check the statistical summary after removing outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data['Value'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data['GDD'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump merged_data to csv to test\n",
    "merged_data.to_csv('merged_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize Correlation Between GDD and Corn Yield**\n",
    "----------------------------------------------------\n",
    "Create a scatter plot to visualize the correlation, with a color gradient showing time progression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot with color gradient for time progression\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Scatter plot with color mapped to 'year' and a plasma colormap for better color representation\n",
    "scatter = plt.scatter(x=merged_data['GDD'], y=merged_data['Value'], c=merged_data['year'], cmap='plasma', s=10)\n",
    "\n",
    "# Add colorbar to show year progression\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_label('Year')\n",
    "\n",
    "# Set titles and labels\n",
    "plt.title('Correlation between GDD and Corn Yield with Time Progression', fontsize=14)\n",
    "plt.xlabel('Growing Degree Days (GDD)', fontsize=12)\n",
    "plt.ylabel('Corn Yield (bu/acre)', fontsize=12)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of GDD vs. corn yield\n",
    "sns.lmplot(x='GDD', y='Value', data=merged_data, scatter_kws={'s': 1})\n",
    "plt.title('Correlation between GDD and Corn Yield')\n",
    "plt.xlabel('Growing Degree Days (GDD)')\n",
    "plt.ylabel('Corn Yield (bu/acre)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyze Precipitation Data**\n",
    "------------------------------\n",
    "We will analyze the correlation between total precipitation (PRCP) and corn yield."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the same chart with PRCP instead of GDD\n",
    "\n",
    "# Calculate total precipitation for the growing season\n",
    "weather_data['PRCP'] = weather_data['PRCP'].clip(lower=0)\n",
    "prcp_annual = weather_data.groupby(['state_ansi', 'county_ansi', 'year'])['PRCP'].sum().reset_index()\n",
    "\n",
    "# Merge on 'state_ansi', 'county_ansi', and 'year'\n",
    "merged_data_prcp = pd.merge(prcp_annual, corn_data, on=['state_ansi', 'county_ansi', 'year'])\n",
    "# Remove entries with zero or NaN yields\n",
    "merged_data_prcp = merged_data_prcp[merged_data_prcp['Value'] > 0]\n",
    "# Remove entries with zero or NaN yields\n",
    "merged_data_prcp = merged_data_prcp[merged_data_prcp['PRCP'] > 0]\n",
    "\n",
    "# Scatter plot with regression line\n",
    "sns.lmplot(x='PRCP', y='Value', data=merged_data_prcp, scatter_kws={'s': 0.5})\n",
    "plt.title('Correlation between PRCP and Corn Yield')\n",
    "plt.xlabel('Total Precipitation (mm)')\n",
    "plt.ylabel('Corn Yield (bu/acre)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the data was similar to the GDD data, we dind't feel it was necessary to include the analysis of the precipitation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add precipitation data to the merged dataframe\n",
    "#prcp_annual = weather_data.groupby(['state_ansi', 'county_ansi', 'year'])['PRCP'].sum().reset_index()\n",
    "#merged_data = pd.merge(merged_data, prcp_annual, on=['state_ansi', 'county_ansi', 'year'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perform Linear Regression Analysis**\n",
    "--------------------------------------\n",
    "We will perform an Ordinary Least Squares (OLS) regression to quantify the relationship between GDD and corn yield."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Prepare the data for OLS (adding a constant for intercept)\n",
    "X = sm.add_constant(merged_data['GDD'])  # Independent variable (GDD) with a constant\n",
    "y = merged_data['Value']  # Dependent variable (Corn Yield)\n",
    "\n",
    "# Step 2: Fit the OLS model\n",
    "ols_model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Step 3: Print out the summary of the regression\n",
    "print(ols_model.summary())\n",
    "\n",
    "# Step 4: Plot the scatter plot with regression line (optional, already in your code)\n",
    "sns.lmplot(x='GDD', y='Value', data=merged_data, scatter_kws={'s': 1})\n",
    "plt.title('Correlation between GDD and Corn Yield')\n",
    "plt.xlabel('Growing Degree Days (GDD)')\n",
    "plt.ylabel('Corn Yield (bu/acre)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multiple Linear Regression**\n",
    "------------------------------\n",
    "Include both GDD and Year as independent variables to see if time has an effect on yield."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Prepare the data for OLS (adding a constant for intercept)\n",
    "X = merged_data[['GDD', 'year']]  # Independent variables (GDD and Year)\n",
    "X = sm.add_constant(X)  # Add constant for the intercept\n",
    "y = merged_data['Value']  # Dependent variable (Corn Yield)\n",
    "\n",
    "# Step 2: Fit the OLS model\n",
    "ols_model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Step 3: Print out the summary of the regression\n",
    "print(ols_model.summary())\n",
    "\n",
    "# Step 4: Plot the scatter plot with regression line for GDD (optional)\n",
    "sns.lmplot(x='GDD', y='Value', data=merged_data, scatter_kws={'s': 1})\n",
    "plt.title('Correlation between GDD and Corn Yield')\n",
    "plt.xlabel('Growing Degree Days (GDD)')\n",
    "plt.ylabel('Corn Yield (bu/acre)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define independent variables and dependent variable\n",
    "#X = merged_data[['GDD', 'PRCP']]\n",
    "X = merged_data[['Value']]\n",
    "y = merged_data['GDD']\n",
    "\n",
    "# Add a constant term for the intercept\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the regression model\n",
    "model = sm.OLS(y, X).fit()\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ranking**\n",
    "------------\n",
    "We determined a ranking of the counties based on a number of factors and will use a normalized score to rank the counties. in the end.\n",
    "\n",
    "- Highest Average Yield\n",
    "- Consistency Above State Average\n",
    "- Highest GDD\n",
    "- Low Variability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average yield per county over the entire period\n",
    "average_yield = merged_data.groupby('county_name')['Value'].mean().reset_index()\n",
    "\n",
    "# Sort counties by average yield in descending order\n",
    "average_yield_sorted = average_yield.sort_values(by='Value', ascending=False)\n",
    "\n",
    "# Top 10 counties\n",
    "print(\"Top 10 Counties by Average Corn Yield:\")\n",
    "print(average_yield_sorted.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizontal bar plot of average yields for top 10 counties in reverse order\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.barh(average_yield_sorted['county_name'].head(10)[::-1], average_yield_sorted['Value'].head(10)[::-1], color='skyblue')\n",
    "plt.ylabel('County')\n",
    "plt.xlabel('Average Corn Yield (bu/acre)')\n",
    "plt.title('Top 10 Counties by Average Corn Yield')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I was wondering if we pick the best based how consistent are they in beating the state average\n",
    "\n",
    "# Calculate the state average yield for each year\n",
    "state_average_yield = merged_data.groupby('year')['Value'].mean().reset_index(name='state_avg_yield')\n",
    "\n",
    "# Merge state average back into merged_data\n",
    "merged_data_with_state_avg = pd.merge(merged_data, state_average_yield, on='year')\n",
    "\n",
    "# Create a flag for whether the county's yield is above the state average each year\n",
    "merged_data_with_state_avg['above_state_avg'] = merged_data_with_state_avg['Value'] > merged_data_with_state_avg['state_avg_yield']\n",
    "\n",
    "# Calculate the percentage of years each county was above the state average\n",
    "county_performance = merged_data_with_state_avg.groupby('county_name')['above_state_avg'].mean().reset_index()\n",
    "\n",
    "# Convert to percentage\n",
    "county_performance['percent_above_state_avg'] = county_performance['above_state_avg'] * 100\n",
    "\n",
    "# Sort counties by percentage in descending order\n",
    "county_performance_sorted = county_performance.sort_values(by='percent_above_state_avg', ascending=False)\n",
    "\n",
    "# Display the top 10 counties\n",
    "print(\"Top 10 Counties by Consistency in Exceeding State Average Yield:\")\n",
    "print(county_performance_sorted.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizontal bar plot of top 10 counties by percentage of years above state average in reverse order\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.barh(county_performance_sorted['county_name'].head(10)[::-1], county_performance_sorted['percent_above_state_avg'].head(10)[::-1], color='orange')\n",
    "plt.ylabel('County')\n",
    "plt.xlabel('Percentage of Years Above State Average (%)')\n",
    "plt.title('Top 10 Counties Consistently Exceeding State Average Yield')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about finding the best improved county over time wiht slope\n",
    "\n",
    "# Prepare a DataFrame to store slopes\n",
    "county_slopes = []\n",
    "\n",
    "for county in merged_data['county_name'].unique():\n",
    "    county_data = merged_data[merged_data['county_name'] == county]\n",
    "    if len(county_data['year'].unique()) > 1:  # Ensure there's enough data\n",
    "        slope, intercept, r_value, p_value, std_err = linregress(county_data['year'], county_data['Value'])\n",
    "        county_slopes.append({'county_name': county, 'slope': slope, 'p_value': p_value})\n",
    "    else:\n",
    "        county_slopes.append({'county_name': county, 'slope': np.nan, 'p_value': np.nan})\n",
    "\n",
    "# Convert to DataFrame\n",
    "slopes_df = pd.DataFrame(county_slopes)\n",
    "\n",
    "# Remove counties with NaN slopes\n",
    "slopes_df.dropna(subset=['slope'], inplace=True)\n",
    "\n",
    "# Sort counties by slope\n",
    "slopes_df_sorted = slopes_df.sort_values(by='slope', ascending=False)\n",
    "\n",
    "print(\"Top 10 Counties by Yield Improvement Over Time:\")\n",
    "print(slopes_df_sorted.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizontal bar plot of top 10 counties by yield improvement over time (slope)\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.barh(slopes_df_sorted['county_name'].head(10)[::-1], slopes_df_sorted['slope'].head(10)[::-1], color='green')\n",
    "plt.ylabel('County')\n",
    "plt.xlabel('Slope of Yield Improvement Over Time')\n",
    "plt.title('Top 10 Counties by Yield Improvement Over Time')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets looks at the counties that have been the most consistent in their yield over time\n",
    "\n",
    "# Calculate standard deviation of yield per county\n",
    "yield_variability = merged_data.groupby('county_name')['Value'].std().reset_index(name='yield_std')\n",
    "\n",
    "# Sort counties by yield_std in ascending order\n",
    "yield_variability_sorted = yield_variability.sort_values(by='yield_std')\n",
    "\n",
    "print(\"Top 10 Counties with Least Yield Variability:\")\n",
    "print(yield_variability_sorted.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizontal bar plot of top 10 counties with least yield variability (standard deviation)\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.barh(yield_variability_sorted['county_name'].head(10)[::-1], yield_variability_sorted['yield_std'].head(10)[::-1], color='purple')\n",
    "plt.ylabel('County')\n",
    "plt.xlabel('Yield Variability (Standard Deviation)')\n",
    "plt.title('Top 10 Counties with Least Yield Variability (Reversed Order)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look at the counties with the highest average gdd\n",
    "\n",
    "# Calculate average GDD per county\n",
    "avg_gdd_per_county = merged_data.groupby('county_name')['GDD'].mean().reset_index()\n",
    "\n",
    "# Sort counties by average GDD in descending order\n",
    "avg_gdd_per_county_sorted = avg_gdd_per_county.sort_values(by='GDD', ascending=False)\n",
    "\n",
    "print(\"Top 10 Counties by Average Growing Degree Days:\")\n",
    "print(avg_gdd_per_county_sorted.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizontal bar plot of top 10 counties by average GDD\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.barh(avg_gdd_per_county_sorted['county_name'].head(10)[::-1], avg_gdd_per_county_sorted['GDD'].head(10)[::-1], color='red')\n",
    "plt.ylabel('County')\n",
    "plt.xlabel('Average Growing Degree Days (GDD)')\n",
    "plt.title('Top 10 Counties by Average Growing Degree Days')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use county_name to merge all the other DFs\n",
    "county_metrics = average_yield[['county_name', 'Value']].merge(\n",
    "    county_performance[['county_name', 'percent_above_state_avg']], on='county_name').merge(\n",
    "    yield_variability[['county_name', 'yield_std']], on='county_name').merge(\n",
    "    avg_gdd_per_county[['county_name', 'GDD']], on='county_name')\n",
    "\n",
    "# Normalize the metrics\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "county_metrics[['avg_yield_norm', 'percent_above_avg_norm', 'yield_std_norm', 'avg_gdd_per_county']] = scaler.fit_transform(\n",
    "    county_metrics[['Value', 'percent_above_state_avg', 'yield_std', 'GDD']])\n",
    "\n",
    "# Invert yield_std_norm so that lower variability scores higher\n",
    "county_metrics['yield_std_norm'] = 1 - county_metrics['yield_std_norm']\n",
    "\n",
    "# I assigned weights to each metric but nothing scientific\n",
    "# Should come back to this later\n",
    "county_metrics['total_score'] = (\n",
    "    county_metrics['avg_yield_norm'] * 0.4 +\n",
    "    county_metrics['percent_above_avg_norm'] * 0.3 +\n",
    "    county_metrics['yield_std_norm'] * 0.1 +\n",
    "    county_metrics['avg_gdd_per_county'] * 0.1\n",
    ")\n",
    "\n",
    "# Sort counties by total_score\n",
    "county_metrics_sorted = county_metrics.sort_values(by='total_score', ascending=False)\n",
    "\n",
    "print(\"Top 10 Counties by Composite Score:\")\n",
    "print(county_metrics_sorted[['county_name', 'total_score']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot of top 10 counties by composite score\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(county_metrics_sorted['county_name'].head(10), county_metrics_sorted['total_score'].head(10), color='purple')\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('County')\n",
    "plt.ylabel('Composite Score')\n",
    "plt.title('Top 10 Counties by Composite Score')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for heatmap\n",
    "metrics_for_heatmap = county_metrics_sorted[['county_name', 'avg_yield_norm', 'percent_above_avg_norm', 'yield_std_norm', 'avg_gdd_per_county', 'total_score']].set_index('county_name')\n",
    "\n",
    "# Rename the columns to make the metric names more readable\n",
    "metrics_for_heatmap.rename(columns={\n",
    "    'avg_yield_norm': 'Normalized Avg Yield',\n",
    "    'percent_above_avg_norm': 'Percent Years Above Avg Yield',\n",
    "    'yield_std_norm': 'Normalized Yield Variability',\n",
    "    'avg_gdd_per_county': 'Avg Growing Degree Days (GDD)',\n",
    "    'total_score': 'Total Score'\n",
    "}, inplace=True)\n",
    "\n",
    "# Set a larger and professional style\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plot heatmap with enhancements\n",
    "heatmap = sns.heatmap(metrics_for_heatmap.head(10), \n",
    "                      annot=True, \n",
    "                      cmap='coolwarm',  # More polished colormap\n",
    "                      linewidths=0.5,   # Add gridlines\n",
    "                      linecolor='black', # Gridline color\n",
    "                      annot_kws={\"size\": 10},  # Annotation text size\n",
    "                      cbar_kws={'label': 'Normalized Values'})  # Label the color bar\n",
    "\n",
    "# Enhance title and axis labels\n",
    "plt.title('Normalized Metrics and Total Score for Top 10 Counties', fontsize=16, weight='bold', pad=20)\n",
    "plt.xlabel('Metrics', fontsize=12)\n",
    "plt.ylabel('County', fontsize=12)\n",
    "\n",
    "# Add a descriptive legend for x-axis values\n",
    "plt.figtext(0.46, -0.1,\n",
    "            'Normalized Avg Yield: Normalized average corn yield per county\\n'\n",
    "            'Percent Years Above Avg Yield: Percent of years the county yield exceeded the state average\\n'\n",
    "            'Normalized Yield Variability: Lower values indicate more consistent yields\\n'\n",
    "            'Avg Growing Degree Days (GDD): Average GDD needed for corn production\\n'\n",
    "            'Total Score: Composite score from the weighted sum of all metrics',\n",
    "            ha='center', fontsize=10, wrap=True, bbox={\"facecolor\":\"lightgray\", \"alpha\":0.5, \"pad\":5})\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.xticks(rotation=30, ha='right', fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the heatmap\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
