{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all the data/usda_* into one dataframe\n",
    "usda = pd.concat([pd.read_csv(f'data/usda_data_{year}.csv')  for year in range(1950, 2024)])\n",
    "\n",
    "# combine all the data/weather_* into one dataframe\n",
    "weather = pd.concat([pd.read_csv(f'data/weather_data_{year}.csv') for year in range(2000, 2021)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usda.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the average value in udsa data for each year\n",
    "usda.groupby('year')['Value'].mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll select the columns that I need for analysis. I will focus on year, county_name and Value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the average value in udsa data for each year\n",
    "usda.groupby('year')['Value'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usda['year'] = pd.to_datetime(usda['year'],\n",
    "               format='%Y')\n",
    "usda_subset = usda[['year','county_name','Value']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usda_subset['county_name'].nunique() #85 Michigan has 83"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is that the data set has 85 and Michigan only has 83. The additional two are 'OTHER COUNTIES' and 'OTHER (COMBINED) COUNTIES'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usda_subset_other_combined = usda_subset[usda_subset['county_name'] == 'OTHER (COMBINED) COUNTIES']\n",
    "usda_subset_other_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 144 records that contain 'OTHER (COMBINED) COUNTIES'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usda_subset_other = usda_subset[usda_subset['county_name'] == 'OTHER COUNTIES']\n",
    "usda_subset_other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\tyear\tcounty_name\tValue\n",
    "60\t2020-01-01\tOTHER COUNTIES\t151.0\n",
    "57\t2021-01-01\tOTHER COUNTIES\t150.2\n",
    "59\t2022-01-01\tOTHER COUNTIES\t163.2\n",
    "57\t2023-01-01\tOTHER COUNTIES\t166.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 4 records (years) that contain 'OTHER COUNTIES'. We will focus on the year 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usda_other = usda[usda['county_name'] == 'OTHER COUNTIES']\n",
    "usda_other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The asd_desc is null for all 4 records. location_desc for all 4 records is \"MICHIGAN, OTHER COUNTIES\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usda_subset_2020=usda_subset[usda_subset['year'] == '2020-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usda_subset_2020['county_name'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 61 counties in the data set for 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usda_subset_2020['county_name'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "array(['DELTA', 'DICKINSON', 'MENOMINEE', 'ANTRIM', 'BENZIE',\n",
    "       'CHARLEVOIX', 'EMMET', 'GRAND TRAVERSE', 'LEELANAU', 'MANISTEE',\n",
    "       'WEXFORD', 'ALCONA', 'ALPENA', 'IOSCO', 'OGEMAW', 'OTSEGO',\n",
    "       'PRESQUE ISLE', 'MASON', 'MUSKEGON', 'NEWAYGO', 'OCEANA',\n",
    "       'GLADWIN', 'ISABELLA', 'MECOSTA', 'MIDLAND', 'MONTCALM', 'ARENAC',\n",
    "       'BAY', 'HURON', 'SAGINAW', 'SANILAC', 'TUSCOLA', 'ALLEGAN',\n",
    "       'BERRIEN', 'CASS', 'KALAMAZOO', 'KENT', 'OTTAWA', 'VAN BUREN',\n",
    "       'BARRY', 'BRANCH', 'CALHOUN', 'CLINTON', 'EATON', 'HILLSDALE',\n",
    "       'INGHAM', 'IONIA', 'JACKSON', 'ST JOSEPH', 'SHIAWASSEE', 'GENESEE',\n",
    "       'LAPEER', 'LENAWEE', 'LIVINGSTON', 'MACOMB', 'MONROE', 'OAKLAND',\n",
    "       'ST CLAIR', 'WASHTENAW', 'WAYNE', 'OTHER COUNTIES'], dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "sns.histplot(data=usda_subset, x=\"Value\", hue=\"county_name\", legend=False)\n",
    "plt.xlabel('BU/Acre')\n",
    "plt.title('Bushels/Acre by County for Past 2 Decades')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust figure size for better readability\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Create the scatter plot\n",
    "g = sns.scatterplot(data=usda_subset_2020, x=\"county_name\", y=\"Value\", hue=\"year\")\n",
    "\n",
    "# Rotate the x-axis labels for readability\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Counties')\n",
    "plt.ylabel('BU/Acre')\n",
    "plt.title('Bushels/Acre by County for 2020')\n",
    "\n",
    "# Adjust the legend positioning\n",
    "#g.legend(loc='center left', bbox_to_anchor=(1.25, 0.5), ncol=1)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that perhaps the \"Other Counties\" was a dumping ground for smaller quanities collected over several counties. There is only one dot respresenting the one entry for the year 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was there really 3 entries for 1960 for \"Other (Combined) Counties\"? A dataset containing the 1960 entries will be created and investigated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usda_subset_1960=usda_subset[usda_subset['year'] == '1960-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust figure size for better readability\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Create the scatter plot\n",
    "g = sns.scatterplot(data=usda_subset_1960, x=\"county_name\", y=\"Value\", hue=\"year\")\n",
    "\n",
    "# Rotate the x-axis labels for readability\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Counties')\n",
    "plt.ylabel('BU/Acre')\n",
    "plt.title('Bushels/Acre by County for 1960')\n",
    "\n",
    "# Adjust the legend positioning\n",
    "#g.legend(loc='center left', bbox_to_anchor=(1.25, 0.5), ncol=1)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure enough there are three dots over the \"Other (Combined) Counties\" mark. What does it look like when we look at the entered values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usda_1960 = usda[(usda['year'] == '1960-01-01') & (usda['county_name'] == 'OTHER (COMBINED) COUNTIES')]\n",
    "usda_1960"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 3 entries for 1960 have the following asd_desc: \"UPPER PENINSULA\", \"NORTHWEST\", and \"NORTHEAST\". The location_desc has these entries: \"MICHIGAN, UPPER PENINSULA, OTHER (COMBINED) COUNTIES\", \"MICHIGAN, NORTHWEST, OTHER (COMBINED) COUNTIES\", and \"MICHIGAN, NORTHEAST, OTHER (COMBINED) COUNTIES\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = usda_subset_1960.groupby('county_name')['Value'].sum().reset_index()\n",
    "print(grouped_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total Value for 1960 \"OTHER (COMBINED) COUNTIES\" was 130.6\n",
    "A similiar dumping ground value when compared to the single \"OTHER COUNTIES\" in 2020.\n",
    "\n",
    "2019 seemed to have a lot of entries for \"OTHER (COMBINED) COUNTIES\". What is going on?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usda_2019 = usda[(usda['year'] == '2019-01-01') & (usda['county_name'] == 'OTHER (COMBINED) COUNTIES')]\n",
    "usda_2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 7 entries for 2019 have the following asd_desc: \"UPPER PENINSULA\", \"NORTHWEST\", \"NORTHEAST\",\"WEST CENTRAL\", \"CENTRAL\", \"SOUTHWEST\", and \"SOUTH CENTRAL\". The location_desc has these entries: \"MICHIGAN, UPPER PENINSULA, OTHER (COMBINED) COUNTIES\", \"MICHIGAN, NORTHWEST, OTHER (COMBINED) COUNTIES\", \"MICHIGAN, NORTHEAST, OTHER (COMBINED) COUNTIES\",\"MICHIGAN, WEST CENTRAL, OTHER (COMBINED) COUNTIES\", \"MICHIGAN, CENTRAL, OTHER (COMBINED) COUNTIES\", \"MICHIGAN, SOUTHWEST, OTHER (COMBINED) COUNTIES\", and \"MICHIGAN, SOUTH CENTRAL, OTHER (COMBINED) COUNTIES\".\n",
    "\n",
    "How many years and how many entries are we looking at that are like this? It looks like 2019 was the worst with 7 regions identified. The amount of bushels/acre is  886.3. This is considerably more than 1960."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = usda.groupby(['year', 'county_name']).agg(\n",
    "    count=('county_name', 'count'),\n",
    "    sum=('Value', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "filtered_df = grouped_df[grouped_df['count'] > 1]\n",
    "# Sort by 'sum' in descending order\n",
    "filtered_df = filtered_df.sort_values(by='sum', ascending=False)\n",
    "\n",
    "print(filtered_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "             year                county_name  count    sum\n",
    "4768 2019-01-01  OTHER (COMBINED) COUNTIES      7  886.3\n",
    "4610 2016-01-01  OTHER (COMBINED) COUNTIES      6  824.1\n",
    "4551 2015-01-01  OTHER (COMBINED) COUNTIES      6  823.4\n",
    "4670 2017-01-01  OTHER (COMBINED) COUNTIES      5  655.8\n",
    "3942 2005-01-01  OTHER (COMBINED) COUNTIES      5  576.0\n",
    "3638 2000-01-01  OTHER (COMBINED) COUNTIES      6  565.0\n",
    "4001 2006-01-01  OTHER (COMBINED) COUNTIES      5  555.0\n",
    "3589 1999-01-01  OTHER (COMBINED) COUNTIES      5  533.0\n",
    "4061 2007-01-01  OTHER (COMBINED) COUNTIES      6  502.0\n",
    "3881 2004-01-01  OTHER (COMBINED) COUNTIES      5  453.0\n",
    "4120 2008-01-01  OTHER (COMBINED) COUNTIES      5  442.0\n",
    "4493 2014-01-01  OTHER (COMBINED) COUNTIES      4  436.4\n",
    "3820 2003-01-01  OTHER (COMBINED) COUNTIES      4  398.0\n",
    "4717 2018-01-01  OTHER (COMBINED) COUNTIES      3  397.1\n",
    "4248 2010-01-01  OTHER (COMBINED) COUNTIES      3  385.7\n",
    "3464 1997-01-01  OTHER (COMBINED) COUNTIES      4  345.0\n",
    "3691 2001-01-01  OTHER (COMBINED) COUNTIES      6  327.0\n",
    "3757 2002-01-01  OTHER (COMBINED) COUNTIES      3  308.0\n",
    "4183 2009-01-01  OTHER (COMBINED) COUNTIES      3  303.0\n",
    "4314 2011-01-01  OTHER (COMBINED) COUNTIES      3  288.7\n",
    "4432 2013-01-01  OTHER (COMBINED) COUNTIES      2  245.4\n",
    "4377 2012-01-01  OTHER (COMBINED) COUNTIES      3  239.2\n",
    "3530 1998-01-01  OTHER (COMBINED) COUNTIES      3  230.0\n",
    "3406 1996-01-01  OTHER (COMBINED) COUNTIES      3  206.0\n",
    "3340 1995-01-01  OTHER (COMBINED) COUNTIES      2  180.0\n",
    "1232 1967-01-01  OTHER (COMBINED) COUNTIES      3  172.8\n",
    "2190 1980-01-01  OTHER (COMBINED) COUNTIES      2  160.6\n",
    "1128 1965-01-01  OTHER (COMBINED) COUNTIES      3  159.3\n",
    "1180 1966-01-01  OTHER (COMBINED) COUNTIES      3  152.5\n",
    "2044 1978-01-01  OTHER (COMBINED) COUNTIES      2  150.0\n",
    "2117 1979-01-01  OTHER (COMBINED) COUNTIES      2  147.4\n",
    "952  1962-01-01  OTHER (COMBINED) COUNTIES      3  144.8\n",
    "1004 1963-01-01  OTHER (COMBINED) COUNTIES      3  142.2\n",
    "900  1961-01-01  OTHER (COMBINED) COUNTIES      3  139.2\n",
    "848  1960-01-01  OTHER (COMBINED) COUNTIES      3  130.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When comparing to the rest of the entries in 2019. How bad does it look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usda_subset_2019=usda_subset[usda_subset['year'] == '2019-01-01']\n",
    "grouped_df_2019 = usda_subset_2019.groupby(['year', 'county_name']).agg(\n",
    "    count=('county_name', 'count'),\n",
    "    sum=('Value', 'sum')\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust figure size for better readability\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Create the scatter plot\n",
    "g = sns.scatterplot(data=grouped_df_2019, x=\"county_name\", y=\"sum\", hue=\"year\")\n",
    "\n",
    "# Rotate the x-axis labels for readability\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Counties')\n",
    "plt.ylabel('BU/Acre')\n",
    "plt.title('Bushels/Acre by County for 2019')\n",
    "\n",
    "# Adjust the legend positioning\n",
    "#g.legend(loc='center left', bbox_to_anchor=(1.25, 0.5), ncol=1)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the counties for 2019 produced no more than 200 bushels/acre. As expected, \"OTHER (COMBINED) COUNTIES\" is the highest in the chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GDD calculation function\n",
    "def calculate_gdd(df, base_temp=50, upper_temp=86):\n",
    "    \"\"\"\n",
    "    Calculate Growing Degree Days (GDD) for corn.\n",
    "    \"\"\"\n",
    "    df['TMAX'] = df['TMAX'].clip(lower=base_temp, upper=upper_temp)\n",
    "    df['TMIN'] = df['TMIN'].clip(lower=base_temp)\n",
    "    df['TAVG'] = (df['TMAX'] + df['TMIN']) / 2\n",
    "    df['GDD'] = df['TAVG'] - base_temp\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = calculate_gdd(weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare average yield for usda_data_1955 and usda_data_2015\n",
    "usda_1955 = pd.read_csv('data/usda_data_1955.csv')\n",
    "usda_2015 = pd.read_csv('data/usda_data_2015.csv')\n",
    "\n",
    "usda_1955['Value'].mean(), usda_2015['Value'].mean()\n",
    "\n",
    "# whats the std deviation of yield for those years\n",
    "usda_1955['Value'].std(), usda_2015['Value'].std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare total gdd for weather_data_1955 and weather_data_2015 for county_ansi=161\n",
    "weather_2014 = pd.read_csv('data/weather_data_2014.csv')\n",
    "weather_2023 = pd.read_csv('data/weather_data_2023.csv')\n",
    "\n",
    "weather_2014 = calculate_gdd(weather_2014)\n",
    "weather_2023 = calculate_gdd(weather_2023)\n",
    "\n",
    "weather_2014[weather_2014['county_ansi'] == 161]['GDD'].sum(), weather_2023[weather_2023['county_ansi'] == 161]['GDD'].sum()\n",
    "\n",
    "# Do this for all years between 1950 and 1959\n",
    "gdd = []\n",
    "for year in range(1950, 1960):\n",
    "    weather = pd.read_csv(f'data/weather_data_{year}.csv')\n",
    "    weather = calculate_gdd(weather)\n",
    "    gdd.append(weather[weather['county_ansi'] == 161]['GDD'].sum())\n",
    "\n",
    "gdd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine all weather data\n",
    "weather_data = pd.concat([pd.read_csv(f'data/weather_data_{year}.csv') for year in range(1950, 2024)])\n",
    "# Assuming 'weather_data' is your concatenated weather DataFrame for all years\n",
    "weather_data = calculate_gdd(weather_data)\n",
    "weather_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'date' to datetime if it's not already\n",
    "weather_data['date'] = pd.to_datetime(weather_data['date'])\n",
    "\n",
    "# Extract year and month\n",
    "weather_data['year'] = weather_data['date'].dt.year\n",
    "weather_data['month'] = weather_data['date'].dt.month\n",
    "\n",
    "# Aggregate GDD by county and year\n",
    "gdd_annual = weather_data.groupby(['state_ansi', 'county_ansi', 'year'])['GDD'].sum().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all USDA data into a single DataFrame usda_data\n",
    "usda_data = pd.concat([pd.read_csv(f'data/usda_data_{year}.csv') for year in range(1950, 2024)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for the relevant data (e.g., 'YIELD' in 'statisticcat_desc')\n",
    "corn_data = usda_data[usda_data['statisticcat_desc'] == 'YIELD']\n",
    "\n",
    "# Convert 'Value' to numeric, removing any commas or missing values\n",
    "corn_data['Value'] = corn_data['Value'].replace(',', '', regex=True)\n",
    "corn_data = corn_data[corn_data['Value'] != '(D)']  # Remove suppressed data\n",
    "corn_data['Value'] = pd.to_numeric(corn_data['Value'])\n",
    "\n",
    "# Select relevant columns\n",
    "corn_data = corn_data[['state_ansi', 'county_ansi', 'year', 'Value']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge on 'state_ansi', 'county_ansi', and 'year'\n",
    "merged_data = pd.merge(gdd_annual, corn_data, on=['state_ansi', 'county_ansi', 'year'])\n",
    "# Remove entries with zero or NaN yields\n",
    "merged_data = merged_data[merged_data['Value'] > 0]\n",
    "# Remove entries with zero or NaN yields\n",
    "merged_data = merged_data[merged_data['GDD'] > 0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump merged_data to csv\n",
    "merged_data.to_csv('merged_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scatter plot with regression line\n",
    "sns.lmplot(x='GDD', y='Value', data=merged_data)\n",
    "plt.title('Correlation between GDD and Corn Yield')\n",
    "plt.xlabel('Growing Degree Days (GDD)')\n",
    "plt.ylabel('Corn Yield (bu/acre)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the same chart with PRCP instead of GDD\n",
    "\n",
    "# Calculate total precipitation for the growing season\n",
    "weather_data['PRCP'] = weather_data['PRCP'].clip(lower=0)\n",
    "prcp_annual = weather_data.groupby(['state_ansi', 'county_ansi', 'year'])['PRCP'].sum().reset_index()\n",
    "\n",
    "# Merge on 'state_ansi', 'county_ansi', and 'year'\n",
    "merged_data_prcp = pd.merge(prcp_annual, corn_data, on=['state_ansi', 'county_ansi', 'year'])\n",
    "# Remove entries with zero or NaN yields\n",
    "merged_data_prcp = merged_data_prcp[merged_data_prcp['Value'] > 0]\n",
    "# Remove entries with zero or NaN yields\n",
    "merged_data_prcp = merged_data_prcp[merged_data_prcp['PRCP'] > 0]\n",
    "\n",
    "# Calculate the Pearson correlation coefficient\n",
    "correlation_prcp = merged_data_prcp['PRCP'].corr(merged_data_prcp['Value'])\n",
    "print(f\"Pearson correlation coefficient between PRCP and corn yield: {correlation_prcp}\")\n",
    "\n",
    "# Scatter plot with regression line\n",
    "sns.lmplot(x='PRCP', y='Value', data=merged_data_prcp)\n",
    "plt.title('Correlation between PRCP and Corn Yield')\n",
    "plt.xlabel('Total Precipitation (mm)')\n",
    "plt.ylabel('Corn Yield (bu/acre)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate precipitation data\n",
    "prcp_annual = weather_data.groupby(['state_ansi', 'county_ansi', 'year'])['PRCP'].sum().reset_index()\n",
    "merged_data = pd.merge(merged_data, prcp_annual, on=['state_ansi', 'county_ansi', 'year'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Define independent variables and dependent variable\n",
    "X = merged_data[['GDD', 'PRCP']]\n",
    "y = merged_data['Value']\n",
    "\n",
    "# Add a constant term for the intercept\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the regression model\n",
    "model = sm.OLS(y, X).fit()\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down the key components of your OLS (Ordinary Least Squares) regression results:\n",
    "\n",
    "### 1. **Model Overview:**\n",
    "   - **Dep. Variable (Dependent Variable):** This is the variable you're trying to predict, in this case, labeled as \"Value.\"\n",
    "   - **Model:** OLS, meaning this is a simple linear regression model using the least squares method.\n",
    "   - **No. Observations:** 4673 observations (data points) were used in this regression.\n",
    "\n",
    "### 2. **R-squared:**\n",
    "   - **R-squared (0.099):** This indicates that the model explains about **9.9% of the variance** in the dependent variable (\"Value\"). This is a relatively low R-squared value, meaning that most of the variability in the data is not explained by the model.\n",
    "   - **Adj. R-squared (0.098):** This is a slightly adjusted version of R-squared that accounts for the number of predictors in the model. It's still low, meaning the model is not very strong at explaining the data.\n",
    "\n",
    "### 3. **F-statistic and p-value (Prob F-statistic):**\n",
    "   - **F-statistic (255.8):** This tests the overall significance of the model. A higher F-statistic generally means the model is a good fit.\n",
    "   - **Prob(F-statistic) (3.88e-106):** The p-value associated with the F-statistic is incredibly small (close to zero), meaning the model is statistically significant overall. So, even though the model doesn't explain much variance (low R-squared), it's still better than having no model at all.\n",
    "\n",
    "### 4. **Coefficients (coef), t-values, and p-values:**\n",
    "   These are the estimated effects of each independent variable on the dependent variable.\n",
    "   - **const (Intercept) (0.0849):** This is the value of the dependent variable (\"Value\") when all independent variables (GDD, PRCP) are zero. The p-value for this is 0.983, which means it's not statistically significant.\n",
    "   - **GDD (0.0274):** This means that for each unit increase in GDD (an independent variable), the dependent variable increases by approximately 0.0274 units. The p-value is 0.000, so this is a statistically significant effect.\n",
    "   - **PRCP (1.9657):** This means that for each unit increase in PRCP, the dependent variable increases by 1.9657 units. The p-value is also 0.000, indicating a highly significant effect.\n",
    "\n",
    "### 5. **Standard Error (std err):**\n",
    "   - The standard errors give a measure of the variability in the coefficient estimates. For example, the standard error for GDD is 0.002, which indicates a small amount of variability, suggesting a reliable estimate.\n",
    "\n",
    "### 6. **t-value and p-value:**\n",
    "   - **t-value:** This tells you how many standard errors the coefficient is away from zero. Higher absolute t-values indicate that the corresponding coefficient is statistically significant.\n",
    "   - **p-value:** This tells you whether the coefficient is statistically significant. For GDD and PRCP, the p-values are 0.000, meaning both are statistically significant. The intercept is not significant (p-value = 0.983).\n",
    "\n",
    "### 7. **Durbin-Watson (0.422):**\n",
    "   - This statistic tests for autocorrelation in the residuals (errors). A value close to 2 means there is no autocorrelation. Here, 0.422 is quite low, suggesting the possibility of positive autocorrelation, which might need further investigation.\n",
    "\n",
    "### 8. **Omnibus and Jarque-Bera (JB) tests:**\n",
    "   - These are tests for normality of the residuals. The p-values for these tests are very small (Prob(Omnibus) = 0.000, Prob(JB) = 8.09e-35), indicating that the residuals are not normally distributed, which could be a problem depending on the assumptions of your model.\n",
    "\n",
    "### 9. **Condition Number (1.66e+04):**\n",
    "   - A high condition number suggests multicollinearity, meaning that the independent variables are highly correlated. The condition number here (16,600) is quite high, indicating potential multicollinearity issues.\n",
    "\n",
    "### Conclusion:\n",
    "- The model is statistically significant, but it doesn't explain much of the variability in the dependent variable (low R-squared).\n",
    "- The variables **GDD** and **PRCP** have a significant impact on the dependent variable (\"Value\").\n",
    "- However, there may be issues with autocorrelation (Durbin-Watson) and multicollinearity (high condition number), and the residuals don't appear to be normally distributed (Omnibus and JB tests). You might need to address these issues if you want to improve the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming usda_data is your concatenated USDA DataFrame for all years\n",
    "# Ensure 'Value' is cleaned and converted to numeric\n",
    "usda_data['Value'] = usda_data['Value'].replace(',', '', regex=True)\n",
    "usda_data['Value'] = usda_data['Value'].replace({'(D)': np.nan, '(Z)': np.nan, '(NA)': np.nan, '': np.nan})\n",
    "usda_data['Value'] = pd.to_numeric(usda_data['Value'], errors='coerce')\n",
    "\n",
    "# Filter for the relevant data (e.g., 'YIELD' in 'statisticcat_desc')\n",
    "corn_data = usda_data[usda_data['statisticcat_desc'] == 'YIELD']\n",
    "\n",
    "# Select relevant columns, including 'county_name'\n",
    "corn_data = corn_data[['state_ansi', 'county_ansi', 'year', 'Value', 'county_name']]\n",
    "\n",
    "# Drop rows with missing values in 'Value' or 'county_name'\n",
    "corn_data.dropna(subset=['Value', 'county_name'], inplace=True)\n",
    "\n",
    "# Ensure 'year' is integer\n",
    "corn_data['year'] = corn_data['year'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge on 'state_ansi', 'county_ansi', and 'year'\n",
    "merged_data = pd.merge(gdd_annual, corn_data, on=['state_ansi', 'county_ansi', 'year'])\n",
    "\n",
    "# Optional: Drop any rows with missing values\n",
    "merged_data.dropna(subset=['GDD', 'Value'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average yield per county\n",
    "avg_yield_per_county = merged_data.groupby(['county_ansi', 'county_name'])['Value'].mean().reset_index()\n",
    "\n",
    "# Sort and select top 10 counties\n",
    "top_counties = avg_yield_per_county.sort_values(by='Value', ascending=False).head(20)\n",
    "\n",
    "# Filter data for these counties\n",
    "top_county_codes = top_counties['county_ansi'].unique()\n",
    "top_county_names = top_counties['county_name'].unique()\n",
    "top_counties_data = merged_data[merged_data['county_ansi'].isin(top_county_codes)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up the plotting environment\n",
    "sns.set_style('whitegrid')\n",
    "num_counties = len(top_county_codes)\n",
    "fig, axes = plt.subplots(nrows=num_counties, ncols=1, figsize=(12, num_counties * 4), sharex=True)\n",
    "\n",
    "# Ensure axes is iterable\n",
    "if num_counties == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, county_code, county_name in zip(axes, top_county_codes, top_county_names):\n",
    "    county_data = top_counties_data[top_counties_data['county_ansi'] == county_code]\n",
    "    county_data = county_data.sort_values('year')\n",
    "\n",
    "    # Plot Yield\n",
    "    ax.plot(county_data['year'], county_data['Value'], color='blue', marker='o', label='Corn Yield (bu/acre)')\n",
    "    ax.set_ylabel('Corn Yield (bu/acre)', color='blue')\n",
    "    ax.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "    # Create a second y-axis for GDD\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.plot(county_data['year'], county_data['GDD'], color='green', marker='x', label='GDD')\n",
    "    ax2.set_ylabel('GDD', color='green')\n",
    "    ax2.tick_params(axis='y', labelcolor='green')\n",
    "\n",
    "    # Title with county name\n",
    "    ax.set_title(f'Corn Yield and GDD Over Time - {county_name}')\n",
    "\n",
    "    # Add legends\n",
    "    lines, labels = ax.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax.legend(lines + lines2, labels + labels2, loc='upper left')\n",
    "\n",
    "# Set common x-label\n",
    "plt.xlabel('Year')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
